{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OQAshK_Cq4vk"
      },
      "source": [
        "# NLP Project\n",
        "\n",
        "Urdu to Roman Urdu Transliterator\n",
        "\n",
        "\n",
        "Help Reference:\n",
        "https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLL5JHqwrEjl"
      },
      "source": [
        "### This notebook contains the data cleaning and tokenization for our models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1aVA33oYraLR"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import string\n",
        "import unicodedata\n",
        "from collections import defaultdict\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import csv\n",
        "import os\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkd_G5bfrCgh",
        "outputId": "ee2045f7-1c0d-46ee-ff05-356bca6e8270"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Es2ox0dVrN5b"
      },
      "outputs": [],
      "source": [
        "# load datasets\n",
        "with open('/content/drive/MyDrive/NLP Project/Urdu.txt', 'r') as f:\n",
        "  urdu = f.read()\n",
        "  urdu = urdu.split('\\n')\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP Project/Roman-Urdu.txt', 'r') as f:\n",
        "  roman_urdu = f.read()\n",
        "  roman_urdu = roman_urdu.split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X_jWdIB-rQie"
      },
      "outputs": [],
      "source": [
        "# only using 60% of the data due to computational constraints\n",
        "urdu, discard_urdu, roman_urdu, discard_roman = train_test_split(urdu, roman_urdu, test_size=0.4, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T9spS4aSrdpP"
      },
      "outputs": [],
      "source": [
        "# split dataset to train-val-test set with ratio of 90%, 10%\n",
        "train_urdu, test_urdu, train_roman, test_roman = train_test_split(urdu, roman_urdu, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_q4aU7SrmCS",
        "outputId": "6a9907b8-4a6a-43d3-d6a7-38319b862af0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "664293 664293 597863 597863 66430 66430\n",
            "پھر وہ کلہاڑی ساتھ لے\n",
            "phir woh kulhari sath le\n"
          ]
        }
      ],
      "source": [
        "# get a look at the sizes of each dataset\n",
        "print(len(urdu), len(roman_urdu), len(train_urdu), len(train_roman), len(test_urdu), len(test_roman))\n",
        "# an example of urdu and its transliteration to roman urdu\n",
        "print(train_urdu[10])\n",
        "print(train_roman[10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Cj8kYpSirvWJ"
      },
      "outputs": [],
      "source": [
        "# preprocess data\n",
        "\n",
        "# convert unicode representation to ascii - function as its taken directly from website\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "train_roman = [unicode_to_ascii(sentence) for sentence in train_roman]\n",
        "train_urdu = [unicode_to_ascii(sentence) for sentence in train_urdu]\n",
        "test_roman = [unicode_to_ascii(sentence) for sentence in test_roman]\n",
        "test_urdu = [unicode_to_ascii(sentence) for sentence in test_urdu]\n",
        "\n",
        "# add white space between punctuation marks so that tokenization is better\n",
        "train_roman = [re.sub(f'([{string.punctuation}])', r\" \\1 \", sentence) for sentence in train_roman]\n",
        "train_roman = [re.sub(r'[\" \"]+', \" \", sentence) for sentence in train_roman]\n",
        "\n",
        "train_urdu = [re.sub(f'([{string.punctuation}])', r\" \\1 \", sentence) for sentence in train_urdu]\n",
        "train_urdu = [re.sub(r'[\" \"]+', \" \", sentence) for sentence in train_urdu]\n",
        "\n",
        "test_roman = [re.sub(f'([{string.punctuation}])', r\" \\1 \", sentence) for sentence in test_roman]\n",
        "test_roman = [re.sub(r'[\" \"]+', \" \", sentence) for sentence in test_roman]\n",
        "\n",
        "test_urdu = [re.sub(f'([{string.punctuation}])', r\" \\1 \", sentence) for sentence in test_urdu]\n",
        "test_urdu = [re.sub(r'[\" \"]+', \" \", sentence) for sentence in test_urdu]\n",
        "\n",
        "# change everything that isnt alphanumeric (or alphanumeric in urdu) or isnt in string.punctuation as space\n",
        "train_roman = [re.sub(r\"[^a-zA-Z0-9{}]+\".format(re.escape(string.punctuation)), \" \", sentence) for sentence in train_roman] \n",
        "test_roman = [re.sub(r\"[^a-zA-Z0-9{}]+\".format(re.escape(string.punctuation)), \" \", sentence) for sentence in test_roman]\n",
        "\n",
        "urdu_alphabet = [\n",
        "    '\\u0627', '\\u0622', '\\u0628', '\\u067E', '\\u062A', '\\u0679', '\\u062B',\n",
        "    '\\u062C', '\\u0686', '\\u062D', '\\u062E', '\\u062F', '\\u0688', '\\u0630',\n",
        "    '\\u0631', '\\u0691', '\\u0632', '\\u0698', '\\u0633', '\\u0634', '\\u0635',\n",
        "    '\\u0636', '\\u0637', '\\u0638', '\\u0639', '\\u063A', '\\u0641', '\\u0642',\n",
        "    '\\u06A9', '\\u06AF', '\\u0644', '\\u0645', '\\u0646', '\\u06BA', '\\u0648',\n",
        "    '\\u06C1', '\\u06BE', '\\u0621', '\\u06CC', '\\u0626', '\\u0624', '\\u0649',\n",
        "    '\\u06D2', '\\u0651', '\\u0670'\n",
        "]\n",
        "\n",
        "urdu_digits = [\n",
        "    '\\u0660', '\\u0661', '\\u0662', '\\u0663', '\\u0664',\n",
        "    '\\u0665', '\\u0666', '\\u0667', '\\u0668', '\\u0669'\n",
        "]\n",
        "\n",
        "train_urdu = [re.sub(r\"[^{}{}{}a-zA-Z0-9]+\".format(''.join(urdu_alphabet), ''.join(urdu_digits), re.escape(string.punctuation)), \" \", sentence) for sentence in train_urdu]\n",
        "test_urdu = [re.sub(r\"[^{}{}{}a-zA-Z0-9]+\".format(''.join(urdu_alphabet), ''.join(urdu_digits), re.escape(string.punctuation)), \" \", sentence) for sentence in test_urdu]\n",
        "\n",
        "# remove trailing and leading whitespace\n",
        "train_roman = [sentence.strip() for sentence in train_roman]\n",
        "train_urdu = [sentence.strip() for sentence in train_urdu]\n",
        "test_roman = [sentence.strip() for sentence in test_roman]\n",
        "test_urdu = [sentence.strip() for sentence in test_urdu]\n",
        "\n",
        "# add a start and eos token to each sentence\n",
        "train_roman = ['<start> ' + sentence + ' <end>' for sentence in train_roman]\n",
        "train_urdu = ['<start> ' + sentence + ' <end>' for sentence in train_urdu]\n",
        "test_roman = ['<start> ' + sentence + ' <end>' for sentence in test_roman]\n",
        "test_urdu = ['<start> ' + sentence + ' <end>' for sentence in test_urdu]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eykNo1jIsDyb",
        "outputId": "ebd1dfca-b791-4595-f1cc-7636084cf34a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> phir woh kulhari sath le <end>\n",
            "<start> پھر وہ کلہاڑی ساتھ لے <end>\n",
            "<class 'str'>\n"
          ]
        }
      ],
      "source": [
        "# the same example data after preprocessing\n",
        "print(train_roman[10])\n",
        "print(train_urdu[10])\n",
        "print(type(train_roman[10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yOQsj_vbsO4h"
      },
      "outputs": [],
      "source": [
        "\n",
        "# make buckets for the different length sentences as done in the paper: 0-10, 10-20, 20-30, 30-40 and > 40\n",
        "bucket_ranges = [10, 20, 30, 40]\n",
        "train_buckets = defaultdict(list)\n",
        "# go over each example in train_roman, train_urdu. if the length of both is < the boundary range then add that example to that range's bucket, else move on\n",
        "for i in range(len(train_roman)):\n",
        "  for j, bucket_range in enumerate(bucket_ranges):\n",
        "    if (len(train_roman[i].split(\" \")) <= bucket_range) and (len(train_urdu[i].split(\" \")) <= bucket_range):\n",
        "      train_buckets[j].append((train_roman[i], train_urdu[i]))\n",
        "      break\n",
        "    # lastly, if u reach the end of the range and u still don't break from this inner loop then add a new bucket for length > 40 and add the example in this bucket\n",
        "    if bucket_range == 40:\n",
        "      train_buckets[j + 1].append((train_roman[i], train_urdu[i]))\n",
        "    \n",
        "# do the same for test data\n",
        "test_buckets = defaultdict(list)\n",
        "for i in range(len(test_roman)):\n",
        "  for j, bucket_range in enumerate(bucket_ranges):\n",
        "    if (len(test_roman[i].split(\" \")) <= bucket_range) and (len(test_urdu[i].split(\" \")) <= bucket_range):\n",
        "      test_buckets[j].append((test_roman[i], test_urdu[i]))\n",
        "      break\n",
        "    if bucket_range == 40:\n",
        "      test_buckets[j + 1].append((test_roman[i], test_urdu[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKZSE0S9sW48",
        "outputId": "da401461-049a-4634-e06f-0f701ada2fdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('<start> musannif Ibn abi rbika ) <end>', '<start> مصنف ابن ابی شیبہ ) <end>')\n",
            "('<start> behtareen dost - 2 , 662 baar <end>', '<start> بہترین دوست - 2 , 662 بار <end>')\n"
          ]
        }
      ],
      "source": [
        "# now each bucket is a 2d list, where each sublist contains tuples of the form (roman_urdu_example, corresponding_urdu_example)\n",
        "# as seen by these examples\n",
        "print(train_buckets[0][1])\n",
        "print(train_buckets[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CP3CgAcSsi8k"
      },
      "outputs": [],
      "source": [
        "# now we make the integer sequences to pass to our models\n",
        "# make tokenizer with no filter and an out of vocab token\n",
        "# fit on entire train_data\n",
        "# then make sequences and add padding for each bucket\n",
        "# do the same for urdu\n",
        "tokenizer_roman = Tokenizer(filters='', oov_token='<OOV>')\n",
        "tokenizer_roman.fit_on_texts(train_roman)\n",
        "tokenizer_urdu = Tokenizer(filters='', oov_token='<OOV>')\n",
        "tokenizer_urdu.fit_on_texts(train_urdu)\n",
        "\n",
        "buckets_train_roman = []\n",
        "buckets_train_urdu = []\n",
        "for i in range(len(train_buckets)):\n",
        "  roman_sentences_in_bucket = [train_buckets[i][j][0] for j in range(len(train_buckets[i]))]\n",
        "  urdu_sentences_in_bucket = [train_buckets[i][j][1] for j in range(len(train_buckets[i]))]\n",
        "  # make integer sequences\n",
        "  roman_sequences = tokenizer_roman.texts_to_sequences(roman_sentences_in_bucket) \n",
        "  # pad the sequences\n",
        "  roman_sequences = pad_sequences(roman_sequences, padding='post')\n",
        "  # save the sequences for this bucket\n",
        "  buckets_train_roman.append(roman_sequences)\n",
        "  # do the same for urdu\n",
        "  urdu_sequences = tokenizer_urdu.texts_to_sequences(urdu_sentences_in_bucket) \n",
        "  urdu_sequences = pad_sequences(urdu_sequences, padding='post')\n",
        "  buckets_train_urdu.append(urdu_sequences)\n",
        "\n",
        "\n",
        "# do the same for test data\n",
        "buckets_test_roman = []\n",
        "buckets_test_urdu = []\n",
        "for i in range(len(test_buckets)):\n",
        "  roman_sentences_in_bucket = [test_buckets[i][j][0] for j in range(len(test_buckets[i]))]\n",
        "  urdu_sentences_in_bucket = [test_buckets[i][j][1] for j in range(len(test_buckets[i]))]\n",
        "  # make integer sequences\n",
        "  roman_sequences = tokenizer_roman.texts_to_sequences(roman_sentences_in_bucket) \n",
        "  # pad the sequences\n",
        "  roman_sequences = pad_sequences(roman_sequences, padding='post')\n",
        "  # save the sequences for this bucket\n",
        "  buckets_test_roman.append(roman_sequences)\n",
        "  # do the same for urdu\n",
        "  urdu_sequences = tokenizer_urdu.texts_to_sequences(urdu_sentences_in_bucket) \n",
        "  urdu_sequences = pad_sequences(urdu_sequences, padding='post')\n",
        "  buckets_test_urdu.append(urdu_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvEt9E08s8UY"
      },
      "source": [
        "The output of this notebook is now in four 2d lists: \n",
        "*   buckets_train_roman\n",
        "*   buckets_train_urdu\n",
        "*   buckets_test_roman\n",
        "*   buckets_test_urdu\n",
        "\n",
        "where each bucket is a 2d list of size 5 for each of the buckets as given in the paper: \"sentence length of 0-10, 10-20, 20-30, 30-40 and > 40\". Each sub-list inside the buckets contains the embeddings / sequences of all the sentences of that size. So, for example, buckets_train_roman[0] contains all padded sequences of Roman Urdu sentences that have size <= 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoiM3erEvU-D",
        "outputId": "d9ac8a7d-a38b-45ed-d085-03905f0a42e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[    2   570   302    26   150    17 15545   109     3     0]\n",
            "[    2   591   309    61   154    17 16362   112     3     0]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[    2,   570,   302, ...,   109,     3,     0],\n",
              "       [    2,  1439,   339, ...,     0,     0,     0],\n",
              "       [    2,  2438,   960, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [    2,    23,   118, ...,    14,     3,     0],\n",
              "       [    2,    30,    45, ...,  1497,     3,     0],\n",
              "       [    2, 12537,    79, ...,     0,     0,     0]], dtype=int32)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# example\n",
        "print(buckets_train_roman[0][0])\n",
        "print(buckets_train_urdu[0][0])\n",
        "# as seen in the output, each sentence has been converted into a padded array of integers.\n",
        "buckets_train_roman[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "S5qp5ipt1j_T"
      },
      "outputs": [],
      "source": [
        "# store the tokenizers as they are needed for the models\n",
        "tokenizer_roman_string = pickle.dumps(tokenizer_roman)\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP Project/tokenizer_roman.pkl', 'wb') as f:\n",
        "    f.write(tokenizer_roman_string)\n",
        "\n",
        "tokenizer_urdu_string = pickle.dumps(tokenizer_urdu)\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP Project/tokenizer_urdu.pkl', 'wb') as f:\n",
        "    f.write(tokenizer_urdu_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "R-TeYuiFGrK-"
      },
      "outputs": [],
      "source": [
        "# store the test data as they are needed later on for the predictions\n",
        "with open('/content/drive/MyDrive/NLP Project/test_buckets.pickle', 'wb') as f:\n",
        "    pickle.dump(test_buckets, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzTm-_YDs0KZ",
        "outputId": "1828b91a-fb23-4412-c1c0-540abea82d90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "file writing completed!\n"
          ]
        }
      ],
      "source": [
        "# store the buckets onto Gdrive to be processed by the models\n",
        "\n",
        "# make a new directory to keep organized \n",
        "new_directory = \"cleaned_data\"\n",
        "parent_directory = \"/content/drive/MyDrive/NLP Project/\"\n",
        "\n",
        "path = os.path.join(parent_directory, new_directory)\n",
        "os.makedirs(path, exist_ok=True)\n",
        "\n",
        "# write lists to csv\n",
        "with open('/content/drive/MyDrive/NLP Project/cleaned_data/buckets_train_roman.csv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for row in buckets_train_roman:\n",
        "        writer.writerow([','.join(map(str, sublist)) for sublist in row])\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP Project/cleaned_data/buckets_train_urdu.csv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for row in buckets_train_urdu:\n",
        "        writer.writerow([','.join(map(str, sublist)) for sublist in row])\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP Project/cleaned_data/buckets_test_roman.csv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for row in buckets_test_roman:\n",
        "        writer.writerow([','.join(map(str, sublist)) for sublist in row])\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP Project/cleaned_data/buckets_test_urdu.csv', 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for row in buckets_test_urdu:\n",
        "        writer.writerow([','.join(map(str, sublist)) for sublist in row])\n",
        "\n",
        "print('file writing completed!')\n",
        "# fin."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
