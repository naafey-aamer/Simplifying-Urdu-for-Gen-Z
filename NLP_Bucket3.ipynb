{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l3CV8dRIyxJM"
      },
      "source": [
        "# NLP Project\n",
        "\n",
        "Urdu to Roman Urdu Transliterator\n",
        "\n",
        "Help Reference:\n",
        "https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgI4xaa7wN43"
      },
      "source": [
        "### This notebook contains the training, evaluation, and testing of the model for sentences of length > 20 and <= 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5GXHbPawpq-",
        "outputId": "f74d6c78-4f98-46a4-ce9d-19ce2fada4da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.4.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.27.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting datasets>=2.0.0\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.7.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets, evaluate\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.22.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.2.0)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24954 sha256=d68fb33ff896c8312d646fd5b06e2e55f08c26b59e3a4558e369843fbe1db4bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "# imports \n",
        "import csv\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import Concatenate, Attention\n",
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow_addons.seq2seq import BasicDecoder\n",
        "from keras.losses import SparseCategoricalCrossentropy\n",
        "import time as time\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import random\n",
        "!pip install evaluate\n",
        "import evaluate\n",
        "!pip install rouge_score\n",
        "import numpy as np\n",
        "import heapq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xubph_IswbCg",
        "outputId": "f5120b0f-df1d-4f4c-e857-6c891d986e18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "35LlhcvSwddg"
      },
      "outputs": [],
      "source": [
        "# load datasets\n",
        "with open('/content/drive/MyDrive/NLP Project/cleaned_data/buckets_train_roman.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    buckets_train_roman = [[[int(num) for num in sublist.split(',')] for sublist in row] for row in reader]\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP Project/cleaned_data/buckets_train_urdu.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    buckets_train_urdu = [[[int(num) for num in sublist.split(',')] for sublist in row] for row in reader]\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP Project/cleaned_data/buckets_test_roman.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    buckets_test_roman = [[[int(num) for num in sublist.split(',')] for sublist in row] for row in reader]\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP Project/cleaned_data/buckets_test_urdu.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    buckets_test_urdu = [[[int(num) for num in sublist.split(',')] for sublist in row] for row in reader]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03IdnXXxxCLq",
        "outputId": "85cfd977-fb1e-4605-aca5-18651a49644f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19902"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# just printing to see if i didnt mess up anything\n",
        "len(buckets_train_roman[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ppw5BjZHzwrc"
      },
      "outputs": [],
      "source": [
        "# ok, i didn't mess up anything - now time to build the tensorflow dataset for the model\n",
        "\n",
        "current_index = 2 # this line is to be changed for each file\n",
        "\n",
        "# hyperparameters for the dataset\n",
        "buffer_size = 32000\n",
        "batch_size = 64\n",
        "\n",
        "# make tensorflow train and test dataset from the buckets, shuffle the train data and convert all datasets to batches of size 64 (as done in the paper)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((buckets_train_urdu[current_index], buckets_train_roman[current_index]))\n",
        "train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((buckets_test_urdu[current_index], buckets_test_roman[current_index]))\n",
        "test_dataset = test_dataset.batch(batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XqGzxUwJ11kK"
      },
      "outputs": [],
      "source": [
        "# load the tokenizers\n",
        "with open('/content/drive/MyDrive/NLP Project/tokenizer_roman.pkl', 'rb') as f:\n",
        "    tokenizer_roman_string = f.read()\n",
        "\n",
        "tokenizer_roman = pickle.loads(tokenizer_roman_string)\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP Project/tokenizer_urdu.pkl', 'rb') as f:\n",
        "    tokenizer_urdu_string = f.read()\n",
        "\n",
        "tokenizer_urdu = pickle.loads(tokenizer_urdu_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yOeKkuDD0-js"
      },
      "outputs": [],
      "source": [
        "# hyperparameters for the model\n",
        "vocab_tar_size = len(tokenizer_roman.word_index) + 1\n",
        "vocab_inp_size = len(tokenizer_urdu.word_index) + 1\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "steps_per_epoch = 17300 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSdxHDda5PWx"
      },
      "source": [
        "## Model Architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8_GCDDTJDSjc"
      },
      "outputs": [],
      "source": [
        "# code used from the help reference and modified a bit to fit our needs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3i5mOeihkZKF"
      },
      "outputs": [],
      "source": [
        "# encoder\n",
        "class Encoder(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, num_layers=3):\n",
        "      super(Encoder, self).__init__()\n",
        "      self.batch_sz = batch_sz\n",
        "      self.enc_units = enc_units\n",
        "      self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "      self.num_layers = num_layers\n",
        "      # bidirectional lstms, number of layers is passed from arguments\n",
        "      self.lstms = [Bidirectional(LSTM(self.enc_units, return_sequences=True, return_state=True)) for i in range(self.num_layers)]\n",
        "\n",
        "  # custom feedforward function\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    states = hidden\n",
        "    new_states = []\n",
        "    # for each bi_lstm layer, pass in the initial_state from the encoder's hidden state and then get the forward activation, concatenate backward and forward activation\n",
        "    for i in range(self.num_layers):\n",
        "        forward_init_state = states[i*4:i*4+2]\n",
        "        backward_init_state = states[i*4+2:i*4+4]\n",
        "        x, forward_h, forward_c, backward_h, backward_c = self.lstms[i](x, initial_state=forward_init_state + backward_init_state)\n",
        "        h = Concatenate()([forward_h, backward_h])\n",
        "        c = Concatenate()([forward_c, backward_c])\n",
        "        new_states.extend([h, c])\n",
        "    return x, new_states\n",
        "\n",
        "  def build_initial_states(self, batch_sz):\n",
        "      return [tf.zeros((batch_sz, self.enc_units)) for _ in range(self.num_layers * 4)]  # 4 initial states (2 for each lstm layer) per layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xvo-GHrUknzt"
      },
      "outputs": [],
      "source": [
        "# decoder\n",
        "class Decoder(Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, num_layers):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.num_layers = num_layers\n",
        "        # lstm layers\n",
        "        self.lstms = [LSTM(self.dec_units, return_sequences=True, return_state=True) for _ in range(self.num_layers)]\n",
        "        # output layer\n",
        "        self.fc = Dense(vocab_size)\n",
        "        # attention layer\n",
        "        self.attention = Attention()\n",
        "\n",
        "    # feedforward function \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # get the context vector from the query asked and the output of the encoder\n",
        "        query = tf.expand_dims(hidden[0], 1)\n",
        "        context_vector = self.attention([query, enc_output])\n",
        "        # add the context vector as input to the decoder\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([context_vector, x], axis=-1)\n",
        "\n",
        "        states = hidden\n",
        "        for i in range(self.num_layers):\n",
        "            x, h, c = self.lstms[i](x, initial_state=states[i*2:i*2+2])\n",
        "            states[i*2:i*2+2] = [h, c]\n",
        "\n",
        "        output = tf.reshape(x, (-1, x.shape[2]))\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, states\n",
        "\n",
        "    def build_initial_states(self, enc_hidden):\n",
        "        return enc_hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uaq0XRNsDLXu"
      },
      "outputs": [],
      "source": [
        "# define Adam optimizer and use params given in paper\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba5QAj4_GD4R"
      },
      "source": [
        "## Train and Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pw9RCe2aDhyg"
      },
      "outputs": [],
      "source": [
        "# define the loss function - code used from help reference\n",
        "def loss_function(real, pred):\n",
        "  # multi-class classification\n",
        "  cross_entropy = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "  loss = cross_entropy(y_true=real, y_pred=pred)\n",
        "  # ignore the effect of padded 0s\n",
        "  mask = tf.logical_not(tf.math.equal(real, 0))  \n",
        "  mask = tf.cast(mask, dtype=loss.dtype)  \n",
        "  loss = mask* loss\n",
        "  loss = tf.reduce_mean(loss)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rsEtgsr9ky0J"
      },
      "outputs": [],
      "source": [
        "# helper function to train each batch\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "    # pass input to encoder, get the output from the encoder and pass as input to the decoder\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([tokenizer_roman.word_index['<start>']] * batch_size, 1)\n",
        "\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "        batch_loss = (loss / int(targ.shape[1]))\n",
        "        # update training parameters (kind of like theta := theta + gradient)\n",
        "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOsaNga1Echm",
        "outputId": "021308d9-2653-4464-d6c8-da5fd02cb5c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total batches: 310\n",
            "Epoch 1 Batch 0 Loss 7.7472\n",
            "Epoch 1 Batch 1 Loss 5.0253\n",
            "Epoch 1 Batch 2 Loss 5.0709\n",
            "Epoch 1 Batch 3 Loss 4.8629\n",
            "Epoch 1 Loss 0.0926\n",
            "Time taken for 1 epoch 285.00 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 4.8499\n",
            "Epoch 2 Batch 1 Loss 4.9051\n",
            "Epoch 2 Batch 2 Loss 4.8924\n",
            "Epoch 2 Batch 3 Loss 4.5534\n",
            "Epoch 2 Loss 0.0860\n",
            "Time taken for 1 epoch 232.78 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.6859\n",
            "Epoch 3 Batch 1 Loss 4.6239\n",
            "Epoch 3 Batch 2 Loss 4.5154\n",
            "Epoch 3 Batch 3 Loss 4.7211\n",
            "Epoch 3 Loss 0.0828\n",
            "Time taken for 1 epoch 232.84 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 4.4455\n",
            "Epoch 4 Batch 1 Loss 4.4014\n",
            "Epoch 4 Batch 2 Loss 4.2268\n",
            "Epoch 4 Batch 3 Loss 4.3540\n",
            "Epoch 4 Loss 0.0798\n",
            "Time taken for 1 epoch 232.61 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 4.3828\n",
            "Epoch 5 Batch 1 Loss 4.3447\n",
            "Epoch 5 Batch 2 Loss 4.2830\n",
            "Epoch 5 Batch 3 Loss 4.4095\n",
            "Epoch 5 Loss 0.0773\n",
            "Time taken for 1 epoch 233.33 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 4.1531\n",
            "Epoch 6 Batch 1 Loss 4.1720\n",
            "Epoch 6 Batch 2 Loss 3.9942\n",
            "Epoch 6 Batch 3 Loss 4.2380\n",
            "Epoch 6 Loss 0.0753\n",
            "Time taken for 1 epoch 233.04 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 4.0860\n",
            "Epoch 7 Batch 1 Loss 4.1386\n",
            "Epoch 7 Batch 2 Loss 4.1158\n",
            "Epoch 7 Batch 3 Loss 4.1531\n",
            "Epoch 7 Loss 0.0735\n",
            "Time taken for 1 epoch 233.37 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 3.9520\n",
            "Epoch 8 Batch 1 Loss 3.8869\n",
            "Epoch 8 Batch 2 Loss 4.1260\n",
            "Epoch 8 Batch 3 Loss 3.7650\n",
            "Epoch 8 Loss 0.0718\n",
            "Time taken for 1 epoch 232.84 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 4.0560\n",
            "Epoch 9 Batch 1 Loss 3.9150\n",
            "Epoch 9 Batch 2 Loss 3.9013\n",
            "Epoch 9 Batch 3 Loss 4.0842\n",
            "Epoch 9 Loss 0.0702\n",
            "Time taken for 1 epoch 233.44 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 3.6535\n",
            "Epoch 10 Batch 1 Loss 3.9203\n",
            "Epoch 10 Batch 2 Loss 3.5024\n",
            "Epoch 10 Batch 3 Loss 3.8331\n",
            "Epoch 10 Loss 0.0686\n",
            "Time taken for 1 epoch 233.87 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 3.7906\n",
            "Epoch 11 Batch 1 Loss 3.7392\n",
            "Epoch 11 Batch 2 Loss 3.6145\n",
            "Epoch 11 Batch 3 Loss 3.7036\n",
            "Epoch 11 Loss 0.0670\n",
            "Time taken for 1 epoch 233.46 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 3.4834\n",
            "Epoch 12 Batch 1 Loss 3.6684\n",
            "Epoch 12 Batch 2 Loss 3.6994\n",
            "Epoch 12 Batch 3 Loss 3.5767\n",
            "Epoch 12 Loss 0.0654\n",
            "Time taken for 1 epoch 232.90 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 3.4457\n",
            "Epoch 13 Batch 1 Loss 3.7831\n",
            "Epoch 13 Batch 2 Loss 3.8373\n",
            "Epoch 13 Batch 3 Loss 3.3891\n",
            "Epoch 13 Loss 0.0637\n",
            "Time taken for 1 epoch 232.89 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 3.4930\n",
            "Epoch 14 Batch 1 Loss 3.5423\n",
            "Epoch 14 Batch 2 Loss 3.4634\n",
            "Epoch 14 Batch 3 Loss 3.3902\n",
            "Epoch 14 Loss 0.0620\n",
            "Time taken for 1 epoch 233.48 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 3.5088\n",
            "Epoch 15 Batch 1 Loss 3.2852\n",
            "Epoch 15 Batch 2 Loss 3.3442\n",
            "Epoch 15 Batch 3 Loss 3.5246\n",
            "Epoch 15 Loss 0.0602\n",
            "Time taken for 1 epoch 233.98 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 3.4127\n",
            "Epoch 16 Batch 1 Loss 3.3301\n",
            "Epoch 16 Batch 2 Loss 3.1334\n",
            "Epoch 16 Batch 3 Loss 3.3012\n",
            "Epoch 16 Loss 0.0584\n",
            "Time taken for 1 epoch 234.04 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 3.0878\n",
            "Epoch 17 Batch 1 Loss 3.2791\n",
            "Epoch 17 Batch 2 Loss 3.2722\n",
            "Epoch 17 Batch 3 Loss 3.1712\n",
            "Epoch 17 Loss 0.0564\n",
            "Time taken for 1 epoch 233.67 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 3.0570\n",
            "Epoch 18 Batch 1 Loss 2.9990\n",
            "Epoch 18 Batch 2 Loss 3.1028\n",
            "Epoch 18 Batch 3 Loss 3.1086\n",
            "Epoch 18 Loss 0.0543\n",
            "Time taken for 1 epoch 233.42 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 2.8334\n",
            "Epoch 19 Batch 1 Loss 2.9396\n",
            "Epoch 19 Batch 2 Loss 2.9925\n",
            "Epoch 19 Batch 3 Loss 2.8759\n",
            "Epoch 19 Loss 0.0520\n",
            "Time taken for 1 epoch 233.14 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 2.7376\n",
            "Epoch 20 Batch 1 Loss 2.6856\n",
            "Epoch 20 Batch 2 Loss 2.6849\n",
            "Epoch 20 Batch 3 Loss 2.7900\n",
            "Epoch 20 Loss 0.0496\n",
            "Time taken for 1 epoch 233.59 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 2.6480\n",
            "Epoch 21 Batch 1 Loss 2.6484\n",
            "Epoch 21 Batch 2 Loss 2.5633\n",
            "Epoch 21 Batch 3 Loss 2.7165\n",
            "Epoch 21 Loss 0.0470\n",
            "Time taken for 1 epoch 233.80 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 2.4130\n",
            "Epoch 22 Batch 1 Loss 2.4671\n",
            "Epoch 22 Batch 2 Loss 2.5159\n",
            "Epoch 22 Batch 3 Loss 2.4583\n",
            "Epoch 22 Loss 0.0444\n",
            "Time taken for 1 epoch 233.34 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 2.4874\n",
            "Epoch 23 Batch 1 Loss 2.2941\n",
            "Epoch 23 Batch 2 Loss 2.3718\n",
            "Epoch 23 Batch 3 Loss 2.2311\n",
            "Epoch 23 Loss 0.0415\n",
            "Time taken for 1 epoch 233.25 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 2.1116\n",
            "Epoch 24 Batch 1 Loss 2.1769\n",
            "Epoch 24 Batch 2 Loss 2.2198\n",
            "Epoch 24 Batch 3 Loss 2.1167\n",
            "Epoch 24 Loss 0.0386\n",
            "Time taken for 1 epoch 233.33 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 1.9848\n",
            "Epoch 25 Batch 1 Loss 1.9312\n",
            "Epoch 25 Batch 2 Loss 1.9361\n",
            "Epoch 25 Batch 3 Loss 1.9232\n",
            "Epoch 25 Loss 0.0356\n",
            "Time taken for 1 epoch 233.21 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.8633\n",
            "Epoch 26 Batch 1 Loss 1.8087\n",
            "Epoch 26 Batch 2 Loss 1.7065\n",
            "Epoch 26 Batch 3 Loss 1.7906\n",
            "Epoch 26 Loss 0.0328\n",
            "Time taken for 1 epoch 233.47 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.6189\n",
            "Epoch 27 Batch 1 Loss 1.6893\n",
            "Epoch 27 Batch 2 Loss 1.7285\n",
            "Epoch 27 Batch 3 Loss 1.7231\n",
            "Epoch 27 Loss 0.0311\n",
            "Time taken for 1 epoch 233.83 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.5769\n",
            "Epoch 28 Batch 1 Loss 1.4905\n",
            "Epoch 28 Batch 2 Loss 1.5347\n",
            "Epoch 28 Batch 3 Loss 1.5437\n",
            "Epoch 28 Loss 0.0275\n",
            "Time taken for 1 epoch 234.01 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 1.5142\n",
            "Epoch 29 Batch 1 Loss 1.5825\n",
            "Epoch 29 Batch 2 Loss 1.3749\n",
            "Epoch 29 Batch 3 Loss 1.4020\n",
            "Epoch 29 Loss 0.0253\n",
            "Time taken for 1 epoch 233.16 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 1.3476\n",
            "Epoch 30 Batch 1 Loss 1.3911\n",
            "Epoch 30 Batch 2 Loss 1.4017\n",
            "Epoch 30 Batch 3 Loss 1.3892\n",
            "Epoch 30 Loss 0.0242\n",
            "Time taken for 1 epoch 233.79 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 1.6672\n",
            "Epoch 31 Batch 1 Loss 1.1949\n",
            "Epoch 31 Batch 2 Loss 1.1872\n",
            "Epoch 31 Batch 3 Loss 1.2650\n",
            "Epoch 31 Loss 0.0225\n",
            "Time taken for 1 epoch 233.88 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 1.0243\n",
            "Epoch 32 Batch 1 Loss 1.2649\n",
            "Epoch 32 Batch 2 Loss 1.0599\n",
            "Epoch 32 Batch 3 Loss 1.1558\n",
            "Epoch 32 Loss 0.0197\n",
            "Time taken for 1 epoch 233.15 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 1.0849\n",
            "Epoch 33 Batch 1 Loss 1.0124\n",
            "Epoch 33 Batch 2 Loss 1.0081\n",
            "Epoch 33 Batch 3 Loss 1.2040\n",
            "Epoch 33 Loss 0.0194\n",
            "Time taken for 1 epoch 233.60 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.8392\n",
            "Epoch 34 Batch 1 Loss 0.9692\n",
            "Epoch 34 Batch 2 Loss 0.9189\n",
            "Epoch 34 Batch 3 Loss 1.2074\n",
            "Epoch 34 Loss 0.0179\n",
            "Time taken for 1 epoch 233.62 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 1.1970\n",
            "Epoch 35 Batch 1 Loss 1.0556\n",
            "Epoch 35 Batch 2 Loss 0.8183\n",
            "Epoch 35 Batch 3 Loss 0.8600\n",
            "Epoch 35 Loss 0.0168\n",
            "Time taken for 1 epoch 233.61 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.7524\n",
            "Epoch 36 Batch 1 Loss 0.7384\n",
            "Epoch 36 Batch 2 Loss 0.7503\n",
            "Epoch 36 Batch 3 Loss 0.7727\n",
            "Epoch 36 Loss 0.0145\n",
            "Time taken for 1 epoch 233.51 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.7259\n",
            "Epoch 37 Batch 1 Loss 0.7120\n",
            "Epoch 37 Batch 2 Loss 0.7468\n",
            "Epoch 37 Batch 3 Loss 0.6667\n",
            "Epoch 37 Loss 0.0133\n",
            "Time taken for 1 epoch 233.45 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.7181\n",
            "Epoch 38 Batch 1 Loss 0.6234\n",
            "Epoch 38 Batch 2 Loss 0.6414\n",
            "Epoch 38 Batch 3 Loss 0.6290\n",
            "Epoch 38 Loss 0.0120\n",
            "Time taken for 1 epoch 233.49 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.6945\n",
            "Epoch 39 Batch 1 Loss 0.5493\n",
            "Epoch 39 Batch 2 Loss 0.6036\n",
            "Epoch 39 Batch 3 Loss 0.6083\n",
            "Epoch 39 Loss 0.0115\n",
            "Time taken for 1 epoch 233.56 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.5819\n",
            "Epoch 40 Batch 1 Loss 0.7096\n",
            "Epoch 40 Batch 2 Loss 0.7129\n",
            "Epoch 40 Batch 3 Loss 0.6934\n",
            "Epoch 40 Loss 0.0139\n",
            "Time taken for 1 epoch 233.58 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.6761\n",
            "Epoch 41 Batch 1 Loss 0.5636\n",
            "Epoch 41 Batch 2 Loss 0.5562\n",
            "Epoch 41 Batch 3 Loss 0.6198\n",
            "Epoch 41 Loss 0.0114\n",
            "Time taken for 1 epoch 233.61 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.7499\n",
            "Epoch 42 Batch 1 Loss 0.5712\n",
            "Epoch 42 Batch 2 Loss 0.6279\n",
            "Epoch 42 Batch 3 Loss 0.5566\n",
            "Epoch 42 Loss 0.0106\n",
            "Time taken for 1 epoch 233.78 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.4965\n",
            "Epoch 43 Batch 1 Loss 0.5219\n",
            "Epoch 43 Batch 2 Loss 0.5014\n",
            "Epoch 43 Batch 3 Loss 0.4740\n",
            "Epoch 43 Loss 0.0089\n",
            "Time taken for 1 epoch 233.82 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.4370\n",
            "Epoch 44 Batch 1 Loss 0.4193\n",
            "Epoch 44 Batch 2 Loss 0.3932\n",
            "Epoch 44 Batch 3 Loss 0.6429\n",
            "Epoch 44 Loss 0.0114\n",
            "Time taken for 1 epoch 233.53 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.5226\n",
            "Epoch 45 Batch 1 Loss 0.4646\n",
            "Epoch 45 Batch 2 Loss 0.4251\n",
            "Epoch 45 Batch 3 Loss 0.4355\n",
            "Epoch 45 Loss 0.0088\n",
            "Time taken for 1 epoch 233.62 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.3885\n",
            "Epoch 46 Batch 1 Loss 0.3839\n",
            "Epoch 46 Batch 2 Loss 0.3831\n",
            "Epoch 46 Batch 3 Loss 0.3503\n",
            "Epoch 46 Loss 0.0074\n",
            "Time taken for 1 epoch 233.67 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.3870\n",
            "Epoch 47 Batch 1 Loss 0.3439\n",
            "Epoch 47 Batch 2 Loss 0.4066\n",
            "Epoch 47 Batch 3 Loss 0.3864\n",
            "Epoch 47 Loss 0.0065\n",
            "Time taken for 1 epoch 233.66 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.4013\n",
            "Epoch 48 Batch 1 Loss 0.2828\n",
            "Epoch 48 Batch 2 Loss 0.3402\n",
            "Epoch 48 Batch 3 Loss 0.2841\n",
            "Epoch 48 Loss 0.0059\n",
            "Time taken for 1 epoch 233.59 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.2888\n",
            "Epoch 49 Batch 1 Loss 0.5143\n",
            "Epoch 49 Batch 2 Loss 1.0164\n",
            "Epoch 49 Batch 3 Loss 0.4371\n",
            "Epoch 49 Loss 0.0099\n",
            "Time taken for 1 epoch 233.52 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.4281\n",
            "Epoch 50 Batch 1 Loss 0.3009\n",
            "Epoch 50 Batch 2 Loss 0.4206\n",
            "Epoch 50 Batch 3 Loss 0.2844\n",
            "Epoch 50 Loss 0.0065\n",
            "Time taken for 1 epoch 233.41 sec\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# main training loop - code modified from help reference\n",
        "import time as time\n",
        "\n",
        "EPOCHS = 50\n",
        "train_loss = []\n",
        "num_layers = 1\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size, num_layers)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units*2, batch_size, num_layers)\n",
        "print('total batches:', len(train_dataset))\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_hidden = encoder.build_initial_states(batch_size)\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        train_loss.append(batch_loss)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print(f'Epoch {epoch+1} Batch {batch // 100} Loss {batch_loss.numpy():.4f}')\n",
        "\n",
        "    print(f'Epoch {epoch+1} Loss {total_loss/steps_per_epoch:.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "987tAUP4_2o4"
      },
      "outputs": [],
      "source": [
        "# beam search:\n",
        "import math \n",
        "\n",
        "def predict_beam(test_bucket):\n",
        "  output = []\n",
        "  # iterate through each example\n",
        "  for i, (roman_sentence, urdu_sentence) in enumerate(test_bucket):\n",
        "    # print(i, \"examples done,\", len(test_bucket) - i, \"remain\")\n",
        "    # convert the sentence to its numeric sequence to give to the model\n",
        "    urdu_sequence = [tokenizer_urdu.word_index[i] if i in tokenizer_urdu.word_index else tokenizer_urdu.word_index['<OOV>'] for i in urdu_sentence.split(' ')]\n",
        "    urdu_sequence = pad_sequences([urdu_sequence], padding='post')\n",
        "    # convert the sequence to a tensor\n",
        "    urdu_sequence = tf.convert_to_tensor(urdu_sequence)\n",
        "    inference_batch_size = urdu_sequence.shape[0]\n",
        "    # call the encoder\n",
        "    enc_start_state = [tf.zeros((inference_batch_size, units)) for _ in range(num_layers * 4)]\n",
        "    enc_out, enc_hidden = encoder(urdu_sequence, enc_start_state)\n",
        "    # create input for decoder\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([tokenizer_roman.word_index['<start>']] * inference_batch_size, 1)\n",
        "    max_len_output = 30\n",
        "    # get the input sequence for the decoder\n",
        "    input_sequence = dec_input.numpy()\n",
        "    # initialize the candidates from beam search, each row is of type: (candidate_sequence, log_probability)\n",
        "    beam_candidates = [[input_sequence, 0.0]]\n",
        "    beam_width = 5\n",
        "    for t in range(max_len_output):\n",
        "      # store the next candidates\n",
        "      next_candidates = []\n",
        "      # for each sequence in the current list of candidates\n",
        "      for seq, score in beam_candidates:\n",
        "        seq = tf.convert_to_tensor(seq)\n",
        "        dec_input = tf.expand_dims(tf.convert_to_tensor([seq.numpy()[0][-1]]) if len(seq) > 0 else tokenizer_roman.word_index['<start>'], 0)\n",
        "        # get the predictions for each token from the decoder for this sequence\n",
        "        predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_out)\n",
        "        # normalize probabilities to prevent negative values\n",
        "        probs = tf.nn.softmax(predictions, axis=1)\n",
        "        min_len = len(roman_sentence.split(' '))\n",
        "        # punish printing <end> before len(roman_sentence) by assigning it a very log value\n",
        "        if t < min_len:\n",
        "          end_token_inx = tokenizer_roman.word_index['<end>']\n",
        "          mask = tf.one_hot(end_token_inx,probs.shape[-1], dtype=tf.float32)\n",
        "          probs = tf.where(mask == 1, tf.ones_like(probs) * -float('inf'), probs)\n",
        "        # store the top k most probable ones - beam search\n",
        "        top_k_preds = tf.math.top_k(probs[0], k=beam_width)\n",
        "        for current_beam in range(beam_width):\n",
        "          predicted_index = top_k_preds.indices[current_beam].numpy()\n",
        "          candidate_sequence = tf.concat([seq, tf.convert_to_tensor([[predicted_index]])], axis=1)\n",
        "          candidate_score = score + math.log(top_k_preds.values[current_beam])\n",
        "          next_candidates.append([candidate_sequence, candidate_score])\n",
        "        \n",
        "      # sort candidates by their score\n",
        "      sorted_candidates = sorted(next_candidates, key=lambda tup: tup[1], reverse=True)\n",
        "      # get the top beam_width candidates\n",
        "      beam_candidates = sorted_candidates[:beam_width]\n",
        "      # exit loop if all candidates have generated <end> token\n",
        "      # if all([tokenizer_roman.index_word[c[0].numpy()[0][-1]] == '<end>' for c in beam_candidates]):\n",
        "      #   break\n",
        "\n",
        "    # best candidate\n",
        "    best_sequence = beam_candidates[0][0]\n",
        "    result = ' '.join([tokenizer_roman.index_word[idx] for idx in best_sequence.numpy()[0]])\n",
        "    output.append((roman_sentence.replace('<start>', '').replace('<end>', '').strip(), result.replace('<start>', '').replace('<end>', '').strip()))\n",
        "  return output\n",
        "\n",
        "# greedy search:\n",
        "def predict_greedy(test_dataset):\n",
        "  preds_bucket = []\n",
        "  # for each example in the test set\n",
        "  for i, tup in enumerate(test_dataset):\n",
        "    # call the evaluate_sentence func on the urdu text's batch\n",
        "    result = evaluate_sentence(tup[1]).replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip()\n",
        "    expected = tup[0].replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip()\n",
        "    preds_bucket.append((expected, result))\n",
        "    if i % 100 == 0:\n",
        "      print(i, \"test examples done,\", len(test_dataset) - i, \"left.\")\n",
        "  print('all done!')\n",
        "  return preds_bucket\n",
        "\n",
        "def evaluate_sentence(sentence):\n",
        "    inputs = [tokenizer_urdu.word_index[i] if i in tokenizer_urdu.word_index else tokenizer_urdu.word_index['<OOV>'] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    inference_batch_size = inputs.shape[0]\n",
        "    result = '<start>'\n",
        "    enc_start_state = [tf.zeros((inference_batch_size, units)) for _ in range(num_layers * 4)] \n",
        "    enc_out, enc_hidden = encoder(inputs, enc_start_state)\n",
        "    \n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([tokenizer_roman.word_index['<start>']] * inference_batch_size, 1)\n",
        "    \n",
        "    for t in range(40):\n",
        "\n",
        "      predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_out) \n",
        "        \n",
        "      # just use argmax to get the token with the highest probability\n",
        "      predicted_id = tf.argmax(predictions, axis=-1).numpy()[0]\n",
        "        \n",
        "      result += tokenizer_roman.index_word[predicted_id] + ' '\n",
        "        \n",
        "      if tokenizer_roman.index_word[predicted_id] == '<end>':\n",
        "          return result.strip()\n",
        "        \n",
        "      # pass the predicted token as the next input to the decoder\n",
        "      dec_input = tf.expand_dims([predicted_id] * inference_batch_size, 1)\n",
        "      \n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FJLg1SkbFkE3"
      },
      "outputs": [],
      "source": [
        "# helper function to calculate bleu score\n",
        "def calculate_bleu_score(preds):\n",
        "  # Split the dataset into expected translations and predicted translations.\n",
        "  expected = [pair[0] for pair in preds]\n",
        "  predicted = [pair[1] for pair in preds]\n",
        "\n",
        "  # # Calculate the BLEU score for each sentence and average BLEU score for all test sentences\n",
        "  bleu_scores = [sentence_bleu([ref.split()], pred.split()) for ref, pred in zip(predicted, expected)]\n",
        "  avg_bleu_score = corpus_bleu([[ref.split()] for ref in expected], [pred.split() for pred in predicted])\n",
        "\n",
        "  return avg_bleu_score * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-E4lf5NWG3J2"
      },
      "outputs": [],
      "source": [
        "def loss_graph(train_loss):\n",
        "  train_steps = list(range(len(train_loss)))\n",
        "  # Create the figure and axis objects\n",
        "  fig, ax = plt.subplots()\n",
        "  # Plot the data\n",
        "  ax.plot(train_steps, train_loss, color='blue')\n",
        "  # Set the title and axis labels\n",
        "  ax.set_title('Training Loss')\n",
        "  ax.set_xlabel('Training Steps')\n",
        "  ax.set_ylabel('Training Loss')\n",
        "  # Show the plot\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "NW6s5tcVIMGJ",
        "outputId": "1e08d790-a58d-4d0b-fdf3-4132bd259204"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot the training loss graph\n",
        "loss_graph(train_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fGIVWIs7GnHj"
      },
      "outputs": [],
      "source": [
        "# load the test data\n",
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/NLP Project/test_buckets.pickle', 'rb') as f:\n",
        "    test_buckets = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA11LwNiGM31",
        "outputId": "308caa9f-03eb-4efb-9658-ea60c5cd7dae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('aur is se pehlay teen hazaar chourasi ya tees so chourasi aur is ke baad teen hazaar chhyasi ya tees so chhyasi bola jata hai',\n",
              "  'aur is novel is hazaar bola jata hai bola jata hai khwaja hai nawaz hai khan hai khan hai , hai , hai , hai pakistan hai   hai'),\n",
              " ('az raah karam aik hoto jo sab se behtar ho aur last ke date ya idiot ke baad ka ho',\n",
              "  'middle jo se aur hum ke ilawa unhon ki katay mein ke ilawa ne mardon ko bntahe ke ki ne kheinchain ke  ke'),\n",
              " ('test 1 : group bi peda 5 jummay England ba Holland ( Lords London ) aur waqt 2 : 30 bujey dopehar',\n",
              "  'mohammad : ae ( waqt dopehar chaltay they jatay they jatay they jatay they jatay they jatay they jatay they jatay they jatay they   they  they'),\n",
              " ('lanch ke chand hi minton mein un dono companiyon ki Website par zabardast Website traffic ki wajah se baith gayeen !',\n",
              "  'july ke azm ki wajah aajay - aa ! - pyare ! yeh miley ga ki miley ga aik day ga ki khayaal   !  !'),\n",
              " ('kya aisa ho sakta hai ke aurat burqa par nah pehnay lekin kapray aisay pehnay ke is ka jism numaya nah ho',\n",
              "  'kya aisa is injaam lekin ne nah ho jaayegaa ho . ne nah ho nah . yeh nah hosakay ke nah ho hai ki  nah   ho'),\n",
              " ('bohat khoob Saood bhai aur Zahir bhai ( Taloot ) ka assar aap par bhi par gaya aglay marhalay mein taranah likhnay ka iradah to nahi',\n",
              "  'bohat khoob bhai jahan bana to chahta to bhala to nahi to bhi ! bhi ! to bhi ! to bhi ! to bhi ! bhi ! to'),\n",
              " ('jo shair mein ne likha hai woh ustaad momin Khan momin ka tumahray barah mein jo momin goya hai is ka nahi',\n",
              "  'jo shair hai faal jo ka mushkil ko tha ne thi ko hai ko - azaiz hai ki tha ko tha ki kis ki'),\n",
              " ('rab Zuljilal inhen ko jawar rehmat mein khusoosi maqam ataa farmaiye avraap o deegar ke ko sabr Jameel ataa farmaiye ( ameeen sm ameeen )',\n",
              "  'rab ko am aur ko ataa ) ameeen ) ) aziz ) aziz ameeen ) aziz ) aziz ) aziz ) aziz ) aziz ) aziz )  aziz )'),\n",
              " ('is ganay par jo meri raye bhi woh aik sandoqchi mein daal kar samandar mein baha di hai',\n",
              "  'is muamlay bhi mein parta ne di ne likhon mein aa ne aa bheja aa di aa jaska di jaska  di'),\n",
              " ('lekin is nazam ko parh kar ki nateeja nikala hai hum ne ke chaar kitaaben parhna to be koi aasaan hai',\n",
              "  'lekin is link hai ke hai bilkul hai thisishypenhere hai thisishypenhere hai araha hai araha hai ki hai thisishypenhere hai ki hai hai  hai'),\n",
              " ('ki aik mukammal quran ka link hai usay khud bhi talawat karen aur apne dosray islami bhaieyon ko bhi usay talawat karne ki targheeb delaina',\n",
              "  'ki aik koshish karen ke hi bani hogi ye hogi ye . ki khatir ki khatir ye hogi ye hogi ki . ye hogi ye hogi ye'),\n",
              " ('qata kalami ki mein maffi chahta hon aik baat kehna chahoon ga kyunkay saal nahi kisi ko bhi kuch kehnay ka sab ko ko marzi ke maalik hain',\n",
              "  'zakhamo pay ziyada kisi hi ke dushman hain shahid hain 236 hain ghalib hain shahid hain bakol ghalib hain shahid hain bakol bakol daagh hain 236 hain ghalib hain 236'),\n",
              " ('karbalaa : scan safha 78 : ( kitaab s 158 - 159 ) : typing mukammal az nayaab',\n",
              "  'karbalaa : ( m me nayaab yaar nayaab ghaznavi / nayaab / nayaab dopehar hain 236 / manay ghaznavi hain'),\n",
              " ('3 . magar ( usay ) is shakhs ke liye nasiyaat ( bana kar utaara ) hai jo ( apne rab se ) darta hai',\n",
              "  '14 . is nataij ) ( liye tha hafiz ) hai ) hai ahbaab ) hai bakol kahan hai ahbaab araha mein hon ) ) ) hai )'),\n",
              " ('oopar wala confirm dekhen aap pa jee mein dhai ghantay ka khitaab suneney ke bilkul mood mein nahi hon',\n",
              "  'oopar wala bajaye jisay ke nahi ndard hon pakistan nahi mein hon mein nahi mein agaya ! mein ! ) hon    hon mein  nahi'),\n",
              " ('aqwam mutahidda ki salamti council ne Iraq ke khilaaf fazai imtina nafiz karne ke liye vote liya cuba mukhalfat mein vote dainay wala wahid malik tha',\n",
              "  'aqwam mutahidda japan karne bhar - e haal zawaal ki tha zawaal tha ne tha ka dosra virginia ki gharelo ka iradah ka barhi - haal tha ne'),\n",
              " ('wazeer Azam aur army cheif ke sath sath naye sikrt bas ke cheif mister are ko bhi madoo kya gaya tha',\n",
              "  'march aur sath ali hi banaya tha mila hai chal raha tha kya gaya gaya hai mila tha kya tha deta rehta thi   raha  raha'),\n",
              " ('America mein ki behan aur beti Afia Siddiqui ko ilzaam mukhtalif mein guzashta kayi saal se paband salasil bangal kar be panah mzalm dhaa raha hai .',\n",
              "  'america mein aur mein oont raha hai kehta hai sakta hai sakta hai sakta hai sakta hai sakta hai lagta hai hai . . hai . hai hai .'),\n",
              " ('is pouday ki phali mein se daal ki bajaye tail se bharpoor beej peda hotay hain jis ki wajah se yeh mumtaz hesiyat rakhta hai',\n",
              "  'is wajdan ki mamooli wajah se bantaa rehta kehlata hai thisishypenhere karbalaa karbalaa hai thisishypenhere bantaa hai thisishypenhere bantaa hai thisishypenhere bantaa hai thisishypenhere beta kehlata hai'),\n",
              " ('aap ahbaab mein se ki uski wazahat karsaktha hai ke ki kaisay kaam karta hai aur kya waqai aisa soft wear hai bhi ya nahi',\n",
              "  'aap ne hai ko mushkil karne ya hon ya nahi bani he bhi ya nahi aur nahi jata hai bani bhi ya bhi jata hai ya hon  hai'),\n",
              " ('Fahim ka Amaar ka mug der se ban kar aay y dono ke taswerain baqyon se kuch choti reh gayeen theen',\n",
              "  'qibla ki der liye se chali . bani gayi . bani gayi . bani gayi . 60061 hain . 60061 jati hain .  jati  .  .'),\n",
              " ('sabiq wazeer mazhabi umoor Punjab Saeed Al Hasan gilani ne kaha ke quran parhnay ka zouq zindah karne ki zaroorat hai',\n",
              "  'feb punjab mein abdullah karne hai thisishypenhere zaroorat hai hogi hogi hogi hogi hogi hogi hogi hai hogi hai hogi hai hogi hai  hai  hai  hai'),\n",
              " ('aap ki pehli ke liye to mujhe Shamshad bhai ki mad leni parre gi uf kitney khtr naak batain kar te hain aap',\n",
              "  'aap ki to shamshad - kar bakol bakol bakol bakol bakol bakol bakol likhte hain bakol bakol bakol bakol pyare bakol bakol bakol huzoor kehta'),\n",
              " ('America ko zaroorat nahi Pakistan ko pathar ke saal mein le jaane ki hum khud se poanch gaye hain',\n",
              "  'america to baad le usay gayeen hain ko gayeen hain ko gayeen hain ko gayeen hain ko gaye hain se hain  gaye hain  gaye hain  hue'),\n",
              " ('takneeki hisaab se to kala baagh dem theek ho sakta hai magar siyasi ya awaami raye kya hai is baray mein',\n",
              "  'fouj to theek magar kya ne mein ne kahan jaska jaska ko ndard ko duniya ko duniya walon ko duniya ko mein ne'),\n",
              " ('? me ray aa sipahi / pehla ray aa ko / seen noon kal te / ja te hay',\n",
              "  'tig air ko ho gaye hain ! 60061 ho ye hai ye ho ye gaye ! 63618 rahay they bhae gaye      ho'),\n",
              " ('milton mesachosist ka Raqba murabba kilomitr hai aur is ki majmoi abadi afraad par mushtamil hai aur meter satah darya se bulandi par waqay hai',\n",
              "  'van ka hai is jahaaz hai se waqay rehta hai waqay hai waqay hai thisishypenhere nateeja waqay hai thisishypenhere waqay hai thisishypenhere waqay waqay hai waqay hai'),\n",
              " ('Farhat Kiani walidain wala sawal pehlay ho chuka hai . jab taq aap woh talaash karen mein baqi jawab deta hon',\n",
              "  'free kabhi ho sakta aap par nahi na karo na ja sakta hon ko ho ! na kere ! ko ho sakta !  !   !  karen'),\n",
              " ('aap ne ki bhi dekha hoga ke mein ne khud bohat baar copy paste kya hai lekin mandarja baala sharait par poora utartay hue',\n",
              "  'aap ne hoga bohat karne se mashgool kardiya ] farmaya ye ho ye ho daalti ho federation kardiye jatay hain par . shaheed ho jatay hain'),\n",
              " ('mein bhi Karachi se hon ( kher ki maamla to ke malik ka hai ) aam logon ki terhan',\n",
              "  'mein bhi ki haal un thi ki cosuon un maghmoom ki larki ki ki nazeer ki yeh aurat ki qisam ki'),\n",
              " ('bilkul janab woh se bhi kaafi ouncha guzarta tha agar sirf woh se guzray to shayar pakar bhi laita hai',\n",
              "  'bilkul janab tha woh der rahen bhi bhala bhi rehti bhi laita hain to hi rahen to miley bhi khayaal hoga bhi  nahi   hai  hain'),\n",
              " ('neechay jadole mein hum is masawaat ke takraar ke do silsila likhte hain jo mamooli doori se shuru hotay hain aik jadole mein aur dosray mein hai',\n",
              "  'qabal hum is khasiyat hotay aik dosray mein hogi mein lekin ne woh ne gana mein woh mein dosray mein hai ne hai ne jab ne jab ne woh'),\n",
              " ('Bharat ka Dehli nah chalay kal klan ko Mohammad Ali jinah ko bhi kisi nah kisi dehsatgardi mein mulawis kardey',\n",
              "  'ke ka nateeja ne bhi ] ] ye farmaya ] ] ] ] ] ] ] ] ] ] ] ] ]'),\n",
              " ('pehla rakha acha hai umeed hai rasta bhi jagah ge aur chalne ka is se bhi behtar saleeqa aa jaye ga',\n",
              "  'fi hai bhi teen hi jaye ga ayen ga ayen ! ameeen ! ameeen ! ameeen ! ameeen ! ameeen ! ameeen ga  ga jaye ga jaye ga'),\n",
              " ('ab chunkay aaj ki jagah ko bohat ziyada adaad ki durusti se pemaiesh karna mumkin nahi is liye hazaar saal baad ka jawab maloom nahi ho sakta',\n",
              "  'ab chunkay ko millat nahi baad nahi parta ho sakta bhi khayaal ho sakta nahi sakta ho sakta nahi sakta ho sakta pata ho sakta ho sakta nahi ho'),\n",
              " ('acha ki aap ne tanz kya hai kyunkay aap se tanz ke ilawa koy umeed ki bhi nahi ja sakti',\n",
              "  'kabhi allah hon ko ki nazar lagta hai ! kehta hai bhi nahi kehta bhi ja sakti nahi thi ! kehta thi  hon  hon  nahi  hon'),\n",
              " ('Akhtar shirani ki ghazlon mein riwayati mazameen ja baja dikhayi dete hain aur beshtar naazon mein bandhay gay mazameen bhi ghazlon mein Dur atay hain',\n",
              "  '2 ka jaiza hain mein pachaas se bhijwaya se hain ko rahay jaska se bolay ko puchhte ko bheja p _ h se puchhte par ga ko'),\n",
              " ('aur un ke ( apne gharrey hue ) shareek ( un se ) kahin ge ke tum hamari ibadat to nahi baghaawat they',\n",
              "  'urdu ke hue ge sahib nahi samjhna aalam hon ) ho jatay hain bantay jao jao ! 37 hotay hain 240 hotay to karte hain   hotay'),\n",
              " ('aisay harf jin mein hamza alehda harf ke tor par istemaal hota hai un ki kuch misalein darj zail hain',\n",
              "  'aisay allag ke istemaal ke rasta paati hain thisishypenhere dushman hain thisishypenhere dushman hain 236 hain thisishypenhere dushman hain thisishypenhere shayar hain'),\n",
              " ('agarchay is nazam ki ashaat se shayad ki mutadid bah se society ko nahi pouncha magar chay baras mein jis',\n",
              "  'fehrist jung se nahi kya ne tons ke shaamil he ka theen shaamil hai shaamil hain ko millti do mein hain mein'),\n",
              " ('qabar aur mamu ke darmain wali deewar ke darwazay mein kharray ho kar mamu ki aur sehan ki tasaveer banayen',\n",
              "  'ya baad ke load aur ki ho ghaznavi ke ho ye hain umeed farma ye farmatay hain ye farmatay hain ye hain'),\n",
              " ('mein ik baar sahih ki bemaar hui bhi hostel mein tab lag pata gaya tha ke mamu se saal bemaar hona kaisa hota hai',\n",
              "  'mein kitni link raha ne assar hai raha hai raha hai raha hai raha hota hai raha hai raha hai hota hai raha hai hota hai  hota hai'),\n",
              " ('aur jee zia ki goad mein pal kar jawan honay walay kar ko is ke dast baazu honay ka tana dete hain . kya kehnay .',\n",
              "  \"aur jee taluqaat ko ' dete hai chal raha . araha nahi . chal raha . . . . kya . bhala . soch chahta hogi nahi .\"),\n",
              " ('federal rizro ne saloon ke baad aakhir - kaar decemeber ko sharah sood mein izafah kya magar sirf feesad',\n",
              "  'kimbl ke darbaar taqseem nahi wajah ahem ziyada meter satah lafz ziyada ahem ziyada ahem ziyada aqliat ziyada ahem ziyada tha  tha  tha   tha'),\n",
              " ('afsanwi majmuon aur navlon ke ilawa tanqeed tahaqeeq aur deegar kayi mozuaat par dr Saleem Akhtar ki mutadid kitaaben shaya ho chuki hain',\n",
              "  'ittehaad _ h aur tehat ho chukay hain bantay hain bantay hain . hain hain hain hain hain hain hain hain hain hain hain hain  hain  hain'),\n",
              " ('is ko ginti mein paintalis bola jata hai is se pehlay chawalees aur is ke baad chhyalis bola jata hai',\n",
              "  'is ko novel aur baad bola jata hai thisishypenhere chahiye karbalaa waqay hai thisishypenhere sahafi mach jata hai thisishypenhere mach jata hai   hai  hai  hai'),\n",
              " ('taham ab taq yeh taayun nahi kya jaska hai yeh dhamaka janoobi Mumbai mein jari karwayyon ka tasalsul hai ya koi allag noiyat ka waqea',\n",
              "  'taham ab kya ke zawaal ka namona ki razamand ki ilaqon ka tha ki beta ka namona ka hai ka beta ki ilaqon ka qiyam yeh haal'),\n",
              " ('Germany ke aik aisay shehar mein jis ki aqwami aik zamane mein barri mashoor rahi he waisay pata chal bhi jaye to naam sukh lijiye ga',\n",
              "  'america ke aik ahem pata mukammal karta bhai ge ga ayen ! ge ga aeye ge bhai ! ga ge ga ayen ge ga ayen aeye ga ge'),\n",
              " ('quied Hussain ke mamu Haq bhai Saeed varsi ke mamu Haq bhai Saeed varsi ka soyam aaj hoga',\n",
              "  'fahmida ke idaray ka matlab hoga mein hai thisishypenhere hoga jab rehti jab rehti rehti rehti rehti rehti rehti hai'),\n",
              " ('ki saari is baat ki nishanain theen ke ab is ne mein baatil ki farmanrawai aur mukammal tehreer parheen',\n",
              "  'maghribi bohat chahiye jinah aur pa ye faiz farma ye ashraf faiz pa pa ] pa pa pa ] ] ]'),\n",
              " ('Mosoof bta rahay hain ke woh ghalti se sarhad paar kar ke aa gaye they aur be gunah hain',\n",
              "  'ne khayaal poori par is hain jaissa bakol bakol saari hain bakol barray hain sab hain bakol bakol bakol sab hain'),\n",
              " ('mein ne kabhi ki socha hi nahi Dehli jab shukriya aa jaye to sochna band ho jata hai',\n",
              "  'pakistan ko der jahaaz hai liya hai bana hai mil jata ho jata ho chuke jata raha jata hai adeeb  hai  hai  hai  ho  ho'),\n",
              " ('unhon ne kaha ke aloodgi globl varmng aur pani ki aloodgi ki islam ne so saal qabal baat ki thi',\n",
              "  'unhon ne aur sultan bhi yeh thi ki thi siwa bhi ki bhi ki bhi ki thi ki thi ki thi yeh'),\n",
              " ('lehaza is tanzeem ne Pakistan ke liye pi ke ke alfaaz mutayyan kiye hain jo Pakistan ko zahir karta hai',\n",
              "  'hai is aik qabeeley hai hoti hai sakta hai sakta hai sakta hai hoti hai hoti hai hoti hai hoti hai sakta hai  hoti hai  hai  hai'),\n",
              " ('sita haal Pakistani nah sahi magar uska ki claim to durust hai ke uski bachi ka baap Pakistani hai',\n",
              "  'quba nah yeh baat ne manate avrab muamlay waghera hogi waghera hai allah hai sahib hai anees hogi avrab hain aur'),\n",
              " ('pahari ilaqa honay ki wajah se zaraat mehdood hai ziyada tar zarayi pedawar mein Zaitoon phal aur phool shaamil hain',\n",
              "  'mshi gun stephen sun - sakay jatay jatay karliya hai ) jatay hain mein bandah jatay jatay jatay jatay hain mein theen     they'),\n",
              " ('mein government high school shikho poora se metric kya aur Sadiq ayjrtn college bahawalpoor mein daakhil hogaye jahan se mein bi',\n",
              "  'mein tipu zabt mein zayed baray mein hafiz hafiz hafiz hafiz hafiz hafiz hafiz hafiz hafiz hafiz hafiz hafiz hafiz hafiz hafiz hafiz'),\n",
              " ('kher asal masla ki hai ke chunkay ab multi national companian Balochistan mein apna asro rasookh barha rahi hain churraya aisay chtkle chodthee rehti hain !',\n",
              "  'kher asal ab al parre jaissa hain ! valslam hain ! kaar hain ! mirza hain ! thisishypenhere hain nazar jaska hain ! shahid hain ! jaska hain !'),\n",
              " ('jab ki user anjanay mein ki bhi aisa lafz likhta he to wahan woh lafz ki jagah aisi ban jaye ga',\n",
              "  'jab ki niyat woh ki jaye ga aayi jaye ga ye jaye ga ye jaye ga bhai ! ameeen ! ameeen ! ameeen   jaye  jaye  jaye'),\n",
              " ('chay nakaat par par baghaawat hue yahan mahol ko jayen banaya jaye to balouch qoum hukmraanon ke sath mazakraat ko liye hai',\n",
              "  'professor par hue banaya shaukat ko farmaya suna na na na na na na na na na na na na na na na na'),\n",
              " ('zabardast photography aur manazair ke umdah akkaasi - mehfhil ke tasaveer ke zamray mein to aap aur askari chaaye hue hain',\n",
              "  'darasal aur gulukar mein aur load hain gaye hain gayeen gaye hain gayeen hain gaye hain gaye hain gayeen gaye hain gaye hain  hue  hue  hue'),\n",
              " ('aur ki ke msjdin ( khaas ) kkhuda ki hain to kkhuda ke sath kisi aur ki ibadat nah karo',\n",
              "  'aur yeh ) shayad doosri nah hain nah puchhte hain ! thisishypenhere nazar manay ! dekho ! puchhte ! thisishypenhere nazar nah   nah  nah   nah'),\n",
              " ('acha to is ka matlab hai ke woh das tasweer ki dastanoon mein kuch nah kuch haqeeqat hai !',\n",
              "  'acha to hai ki bhi lagta sach hai ! - sakti hai ! sach hogi hai to hai ! hogi hai  hai   hon  hai  hai'),\n",
              " ('is ke zaiqay mein aaj taq kisi qisam ki tabdeeli waqay nahi hui balkay roz awwal se aaj taq is ka wohi zayeqa hai',\n",
              "  'is ke khobsorat nahi se aik hui hai yeh parta hai thisishypenhere likhta - khilafa hai thisishypenhere likhta avrab hui lagta zila hai thisishypenhere likhta hai   hai'),\n",
              " ('aasaan alfaaz mein usay yun bayan kya ja sakta hai ke aik dawat mein so maheman hain aur so rotian hain',\n",
              "  'tom mein kya parta - shumaar aur shayar puchhte avrab rahay avrab razamand hain avrab hain jaissa hain bakol puchhte hain . thisishypenhere'),\n",
              " ('is waqt se kayi buland amarat ke mansoobay manzar aam par aachukay hain jin mein se chand zair taamer bhi hain',\n",
              "  'aur se ilaqon walay wali nazar kkhuda thi thisishypenhere chahtay hain thisishypenhere khilta hai thisishypenhere bhi khilta hai thisishypenhere gana rehti thi thisishypenhere'),\n",
              " ('abhi woh kaam shuru hi kar tha ke achanak computer par aik e mil khil gayi jis mein ki hadees likhi bhi',\n",
              "  'abhi woh kar aik tehreer ki nah ke nah ki tafseer ki nazar ne jayen ki nah ka nazar ki ladli farmaya - kaar'),\n",
              " ('is ke sath sath hum un se poucheen ge un ki pasand / napasand un ki ko shakhsiyat ke baray mein sawalaat aur dhair saari batain',\n",
              "  'is ke sath dawa ko aur samajte hain warna saari tasaveer kyun shadi samajte hain moazrat samajte hain thisishypenhere saari log shadi samajte hain warna shadi hain dehli'),\n",
              " ('aur is se pehlay do hazaar aik so athanway ya ikees so athanway aur is ke baad do hazaar do so ya baaes so bola jata hai',\n",
              "  'aur is samandar aur do so satasi zila qutub daftar jata hai thisishypenhere daftar zila waqay hai pakistan hai mutabiq hai taa hai dlhara jata jata hai mein hai'),\n",
              " ('phir se bore ab urdu mehfhil mein pehlay sa josh nahi raha aur nah hi uskay josh lotney ki umeed hai ab',\n",
              "  'phir se unhon nahi aur umeed hogi ab hogi hogi . ab hogi ab hogi ab hogi hogi ab hogi hogi ab hogi .   hai  hai'),\n",
              " ('kyunkay mein be penday ka lota hon har taraf ghoom jaun ga aur baala aakhir - kaar bachay ga kon be penday ka lota',\n",
              "  'kyunkay mein inka kahe kon ka ikhtisaar jaska ka istafsaar jaska ko ahsaan mein yeh istafsaar jaska ko ahsaan ki stits yeh ihtimaam da yeh ihtimaam'),\n",
              " ('is nizaam mein har shakhs ki aamdani se mehsool wusool kya jata hai chahay woh ghareeb ho ya maldaar',\n",
              "  'is ka mozoon chahiye ya dainee hai ya bani hyderabad bani bani theen ya patla zila bani . ya kaha jata       theen'),\n",
              " ('marshal New York ka Raqba murabba kilomitr hai aur is ki majmoi abadi afraad par mushtamil hai aur meter satah samandar se bulandi par waqay hai',\n",
              "  'marshal new york aur ki dafaa waqay bulandi par waqay waqay waqay hai waqay waqay waqay hai utar waqay waqay waqay hai waqay waqay waqay hai waqay hai'),\n",
              " ('aur aakhir - kaar mein aanar ke danay daal kar jaldi se kha len aisa nah ho ke ki aajay',\n",
              "  'aur aakhir na aisa dubai ayen karen ki aayi ki pataayi karen - li jaye ke nahi ki ho ye bhae ayen'),\n",
              " ('agar hum is doosri wahi ke qaail nahi hain to phir mujhe kehnay dijiye ke bakol quran',\n",
              "  'agar aap nahi sakti sahib sla salle ke liye salle allah noon ne ameeen allah noon rehma allah ameeen'),\n",
              " ('bees hazaar maraslay to waqai aisa sang mil jis par mubarakbaad wajib hojati hai hamari mubarakbaad ke baghair ki dhaga gayarah safhaat taq poanch gaya !',\n",
              "  'punjab to mil - sahih se ho ! balouch ho ! ] ho farmaya gaya ho sakay ! khayaal ho ! ] ! bana gaya karte ho !  bana'),\n",
              " ('America ke baaz mahireen ke mutabiq chain ne laser shua ki madad se Amrici jasoosi sayyaron ko andha karne ki koshish bhi ki hai',\n",
              "  'america ke mutabiq yeh karne ki thi ki thi yeh hon yeh hai ki hai ki hon yeh nahi avrab nahi yeh hai yeh hai ki  bhi'),\n",
              " ('akhbar injaam ko shikast dainay ke liye sansani khaiz mawaad jinsi aur juraim par mabni kahaniyan aur aurat ka jism akhbar ki mustaqil khasusiyat bana raha',\n",
              "  'november ko zila mein isi liye istemaal raha aur bana diya aur diya ke diya raha ] kar - khayaal diya - diya dekhaya - diya gaya -  diya'),\n",
              " ('aur Pakistanio ke liye napasandeedah tareen America hai . agar pehlay number se pehlay bhi ki number hota to mein un ko hi deta .',\n",
              "  'aur punjabi hai pehlay bhi mein nah ne hi . ko kkhuda ho un kya un karen ne ho ne pata ne yahan un huien ne nahi'),\n",
              " ('bhai par ka link lagta hai kaam nahi kar raha agar theek kar den to mein bhi nazar ada kar dun',\n",
              "  'hazrat phir kaam bilkul click kar anay kar click kar anay kar click kar saken kar click kar kar click kar dun gi  kar  kar  click kar'),\n",
              " ('ho nahi sakta balkay usay kal taq ho jana chahiye ! Imran ne aik aik lafz par zor day kar kaha !',\n",
              "  'ja sakta kal nahi is waqt kar raha ! bhala kar bhala kar kar ! kar ! bhala karen gi ! bhala karen kaha  kar kar !  kar'),\n",
              " ('Shamshad bhai aa chuke hain aap ka salam mil gaya ke salam bohat nazar bhai alhamdulillah sab acha hai aap sunaein kaisay hain',\n",
              "  'shamshad bhai ka graphics hai ge is liye moazrat karloon aajate karloon na aajate aajate ga ko ga ge aajate ! jaan ga suna na'),\n",
              " ('port gbsn msispi ka Raqba murabba kilomitr hai aur is ki majmoi abadi afraad par mushtamil hai aur meter satah samandar se bulandi par waqay hai',\n",
              "  'lake wood aur ki murabba hai meter bulandi par hai beta waqay hai thisishypenhere waqay waqay hai waqay hai waqay waqay beta waqay hai waqay hai waqay waqay hai'),\n",
              " ('college jung aur jung se mutaliqa aloom ka aalmi shohrat rakhnay wala idaara hai jis mein back waqt afsaraan taleem haasil karte hain',\n",
              "  'college ya musalmanoon hai baad haasil karte hain karte hain ] hain ] sakay karte karte hain karte ho saki karte karte hain qaazi ho  ho   hain'),\n",
              " ('un tamam filmon mein Surriya ko hi ahem kirdaar ada karne ka mauqa mila jis se yeh alamat zahir honay lagi ke woh dio se barri adakara hain',\n",
              "  'un tamam baar neechay isi se hain ko jati woh se . yeh hain kisi se thi thisishypenhere hain sirf qareeb hain thisishypenhere adeeb avrab jami hain se kisi razamand'),\n",
              " ('sdraldin ko to bohat ki aayi jab hum ne inhen bataya kahoon hain uncle ko kis ne kaha tha jald baazi karne ko acha sun - hwa ab',\n",
              "  'humsaiyon ko jab aaya kon karne sun - sakta hai rehti hai thisishypenhere milta hai sun - khilafa hai thisishypenhere rehti rehti sun - khilafa hai tab hoga waqt hai'),\n",
              " ('Yemen teesri ne ka aik intahi pasmandah malik hai jo Ameer khliji mumalik ka Afghanistan kaha ja sakta hai',\n",
              "  'middle is guzargah jo diya hai sakta hai sakta hai sakta hai sakta hai sakta hai sakta hai sakta hai sakta hai  hon  hai sakta hai  hai'),\n",
              " ('is baichari boorhi ne pata nahi kya kya dekha ho ga zamane mein jo ki itni barri baat nahi bhi',\n",
              "  'is bahanay kya kya mein achi nahi - nahi chalta magar nahi bhi hai nahi woh nahi thi mujhe nahi bhi thi  nahi   nahi  nahi'),\n",
              " ('plat aur mera Allah aap ki zabaan mubarak kere ki do inch zamee Pakistan mein hoti to barson idhar nah rehta',\n",
              "  'kawish aur ki click hai raha hai layte raha naan raha hoon raha naan raha jaissa nahi agaya raha ! likhta hai idhar    nahi   rehti'),\n",
              " ('is kitaab haqeeqat toheed o tamam ko aan line parhnay ya down load karne ke liye yahan click karen',\n",
              "  'is kitaab ko saboot yahan click yahan lagey gi yahan click karen ge yahan lagey gi na karen kahin yahan karen  karen  karen   karen  karen'),\n",
              " ('asbaq tareekh Nami ki kitaab aik aik safhay ke mukhtasir mazameen ka majmoa hai jis mein sahaba ke saal se le kar agay ke adwaar par',\n",
              "  'sahaafat tareekh aik malik karon le p _ hnchayahe ke 2012 shaamil ke sath rshidi shaamil ke 2012 mein smira 2012 jaska shaamil karen ne bhijwaya ke 2012'),\n",
              " ('is hamlay ke liye hajhoom ko uksanay ka kaam muqami bhartia jnta party ke rukan ke betay ne kya tha',\n",
              "  'is liye apni gayarah kya unhon nahi ne nahi un nahi un nahi ne nahi un nahi ke nahi ke kya tha'),\n",
              " ('aik klov meter ya is se ziyada to phir ne liye behtar rahay ga is baray mein kuch yeh kitna kharch aeye ga',\n",
              "  'aik behas to liye ne utar aeye ! shahid aeye ! shahid ayen aeye ga ge ye ga ye ayen ga bhai likhte hain ge    aeye'),\n",
              " ('ke jab quran ke naam par nojawan angrezi taleem Yafta aur islam al tabqa ko ki bawar karaya jata hai',\n",
              "  'ke jab paas aur ko chahiye jata hai jata hai jata hai taa jata hai jata hai taa jata hai bani hai   hai  hai  hai'),\n",
              " ('dr mubarak mubarak mand ne kaha ke thar ke koylay ke zakhair tawanai ke husool ke liye 800 saal taq ke liye kaafi hain -',\n",
              "  'dr 7 ke zimn taq kaafi mushkil hain yeh hain thisishypenhere kehta avrab hain . shahid hain avrab kehta chaloon hain . yeh kehta avrab kehta hain'),\n",
              " ('mein ne likha tha ke unwan mein hi jawab hai phir tadween kar di ke kahin ghalib shamat na le ayen meri',\n",
              "  'mein ne blog hai walay deewany plzzzz mujh karloon apkee zik mujh se darny ko kehnay ki moazrat ki kehnay se moazrat ki !'),\n",
              " ('ne mamu mein hai to mujh se chhupa ke rakhtay hain ya ziyada le ke se hain ke ke to yahi kha jaye ga',\n",
              "  'ne mamu bilkul mein bhi lijiye ! ameeen to aeye to bolay bhi saood ! waris ! ameeen to aeye to aziz bhi saood ga bhai  !'),\n",
              " ('is ke ilawa inhen sahtih academy award aur deegar ezazat bhi haasil ho ye jin mein punjabi adab ke liye geyan pith award bhi shaamil hai',\n",
              "  'is ke paighamaat jin mein manzoor thi jaska bhi mein hai hafiz jaska bheja hai mein hai shaamil hai mein to mein bhi mein hai jaska bhi hafiz'),\n",
              " ('ya is terhan ki mein millti maloomat jo ke aap ke khayaal mein vayrlis net work ke liye itni hon zaroor yeh',\n",
              "  'ya is par ke liye na kahi moazrat karen apkee ! ki ! na yeh ! yeh ! ki ! dehli ! yeh !     !'),\n",
              " ('chalein yahan bhi day deti hon bohat bohat mubarak ho aur Allah kere ke un saloon ki tadaad yuhin barhti rahay',\n",
              "  'chalein day . ho - deti hain . 63618 farmaya faateh kehte hain faateh kehte hain kehte hain kehte hain kehte farmatay hain   ho   ho')]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# calculate the prediction\n",
        "preds_bucket_beam = predict_beam(test_buckets[current_index][0:100])\n",
        "preds_bucket_beam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435,
          "referenced_widgets": [
            "3af102dbd85b4b4dbde66c7a1c0ffa31",
            "0dc8f6900fc04ecea590ef06686f7848",
            "aa031f1647be40b5aa27ca007cc468dc",
            "7fc837eae5654b88997eb225d37a9ebf",
            "cd3acb96a2dd4246a478c1ae1409f4d0",
            "a00c7cb101cd4ccd9909e2ef7204f581",
            "9c4a1fc1291b4d47a69915ca40efb9e3",
            "81a89f4406d14f0ea87703355e5da616",
            "356278234f6d4b8ebca3302569f11c79",
            "aa236bb411de48beaf38ceef03f62999",
            "6a3052ed7f3c43afb9ac7d38afbb18d7"
          ]
        },
        "id": "gtqom0c1xd7I",
        "outputId": "caf873cb-9664-4271-c6af-dc424c1be190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "beam search\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3af102dbd85b4b4dbde66c7a1c0ffa31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU SCORE OF BUCKET SIZE 30: 8.970332228305455e-77\n",
            "ROUGE SCORE OF BUCKET SIZE 30: {'rouge1': 0.22412136643911248, 'rouge2': 0.042865896875720684, 'rougeL': 0.20202940239961845, 'rougeLsum': 0.2011694954874833}\n"
          ]
        }
      ],
      "source": [
        "# quantitaive analysis - bleu score of model, rouge score\n",
        "print('beam search')\n",
        "bleu_score = calculate_bleu_score(preds_bucket_beam)\n",
        "rouge = evaluate.load('rouge')\n",
        "expected = [pair[0] for pair in preds_bucket_beam]\n",
        "predicted = [pair[1] for pair in preds_bucket_beam]\n",
        "rouge_score = rouge.compute(predictions=predicted, references=expected)\n",
        "print(\"BLEU SCORE OF BUCKET SIZE 30:\", bleu_score)\n",
        "print(\"ROUGE SCORE OF BUCKET SIZE 30:\", rouge_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlV0tSzL6yJw",
        "outputId": "bb31be14-3910-4775-debd-4057e3f1894e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 test examples done, 100 left.\n",
            "all done!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('aur is se pehlay teen hazaar chourasi ya tees so chourasi aur is ke baad teen hazaar chhyasi ya tees so chhyasi bola jata hai',\n",
              "  'aur is se pehlay teen hazaar chourasi ya tees so chourasi aur is ke baad teen hazaar chhyalis ya tees so chhyalis bola jata hai'),\n",
              " ('az raah karam aik hoto jo sab se behtar ho aur last ke date ya idiot ke baad ka ho',\n",
              "  'kawish _ h mizayil khittay ki hai jo sab se behtar ho aur'),\n",
              " ('test 1 : group bi peda 5 jummay England ba Holland ( Lords London ) aur waqt 2 : 30 bujey dopehar',\n",
              "  'mufti bughdadi : e bi pal ( waqt london aldeen sah ( lords london ) aur waqt 2 : 30 bujey dopehar they'),\n",
              " ('lanch ke chand hi minton mein un dono companiyon ki Website par zabardast Website traffic ki wajah se baith gayeen !',\n",
              "  'july ke chand hi asbaab mein un dono companiyon ki website par zabardast islam aazmi ki wajah se baith gayeen !'),\n",
              " ('kya aisa ho sakta hai ke aurat burqa par nah pehnay lekin kapray aisay pehnay ke is ka jism numaya nah ho',\n",
              "  'kya aisa ho sakta hai ke aurat ikhatay par nah daali lekin jald aisay ashaab ke is ka rawayya numaya nah ho'),\n",
              " ('bohat khoob Saood bhai aur Zahir bhai ( Taloot ) ka assar aap par bhi par gaya aglay marhalay mein taranah likhnay ka iradah to nahi',\n",
              "  'bohat khoob saood bhai aur zahir bhai ( qaisrani ) ka assar aap par bhi par gaya aap sardiyoon mein kitni likhnay ka iradah to nahi to'),\n",
              " ('jo shair mein ne likha hai woh ustaad momin Khan momin ka tumahray barah mein jo momin goya hai is ka nahi',\n",
              "  'jo shair mein ne likha hai woh ustaad momin khan kazmi ka apne mukalama mein jo hindi boltaa hai is ka'),\n",
              " ('rab Zuljilal inhen ko jawar rehmat mein khusoosi maqam ataa farmaiye avraap o deegar ke ko sabr Jameel ataa farmaiye ( ameeen sm ameeen )',\n",
              "  'jaan brother inhen ko jawar rehmat mein khusoosi maqam ataa farmaiye josh o deegar ke ko sabr jameel ataa farmaiye ( ameeen sm ameeen )'),\n",
              " ('is ganay par jo meri raye bhi woh aik sandoqchi mein daal kar samandar mein baha di hai',\n",
              "  'is muamlay par jo meri raye bhi woh aik dukanon mein daal kar samandar mein hatt di'),\n",
              " ('lekin is nazam ko parh kar ki nateeja nikala hai hum ne ke chaar kitaaben parhna to be koi aasaan hai',\n",
              "  'lekin is nazam ko parh kar ki koshish mila hai hum ne ke chaar kitaaben pasand to be koi aasaan hai'),\n",
              " ('ki aik mukammal quran ka link hai usay khud bhi talawat karen aur apne dosray islami bhaieyon ko bhi usay talawat karne ki targheeb delaina',\n",
              "  'ki aik qanoon quran ka elaan hai usay khud bhi jali karen aur apne dosray ke haqayiq ko bhi usay e koshish ki targheeb'),\n",
              " ('qata kalami ki mein maffi chahta hon aik baat kehna chahoon ga kyunkay saal nahi kisi ko bhi kuch kehnay ka sab ko ko marzi ke maalik hain',\n",
              "  'zakhamo pay ki mein maffi chahta hon aik baat hoti batatay ga kyunkay saal nahi kisi ko bhi kuch kehnay ka sab ko ko marzi ke mustahiq hain'),\n",
              " ('karbalaa : scan safha 78 : ( kitaab s 158 - 159 ) : typing mukammal az nayaab',\n",
              "  'karbalaa : scan safha 77 : ( kitaab s tanz - muqabil ) : typing mukammal az'),\n",
              " ('3 . magar ( usay ) is shakhs ke liye nasiyaat ( bana kar utaara ) hai jo ( apne rab se ) darta hai',\n",
              "  '14 . magar ( usay ) is shakhs ke liye nasiyaat ( bana kar ahthyat ) hai jo ( apne rab se ) dekhatii hai'),\n",
              " ('oopar wala confirm dekhen aap pa jee mein dhai ghantay ka khitaab suneney ke bilkul mood mein nahi hon',\n",
              "  'oopar wala dam dekhen aap pa jee mein dhai klov ka feesad suneney ke bilkul mood mein nahi hon'),\n",
              " ('aqwam mutahidda ki salamti council ne Iraq ke khilaaf fazai imtina nafiz karne ke liye vote liya cuba mukhalfat mein vote dainay wala wahid malik tha',\n",
              "  'aqwam mutahidda ki hareefana masawaat ne europe ke khilaaf karwai numa nafiz karne ke liye vote liya e mukhalfat mein vote dainay wala wahid malik wala tha'),\n",
              " ('wazeer Azam aur army cheif ke sath sath naye sikrt bas ke cheif mister are ko bhi madoo kya gaya tha',\n",
              "  'mohammad azam aur yakam shah ke sath sath naye code aman ke saddar asad are ko bhi muhayya kya gaya tha'),\n",
              " ('America mein ki behan aur beti Afia Siddiqui ko ilzaam mukhtalif mein guzashta kayi saal se paband salasil bangal kar be panah mzalm dhaa raha hai .',\n",
              "  'america mein ki behan aur tafseer surah al ko qiyam walay mein ke kayi saal se pehchana chhalak bangal kar be panah yafta jaar raha hai .'),\n",
              " ('is pouday ki phali mein se daal ki bajaye tail se bharpoor beej peda hotay hain jis ki wajah se yeh mumtaz hesiyat rakhta hai',\n",
              "  'is liye ki series mein se daal ki bajaye faaslay se musalsal mamooli qarar hotay hain jis ki wajah se yeh mushkilaat peda rakhta hai'),\n",
              " ('aap ahbaab mein se ki uski wazahat karsaktha hai ke ki kaisay kaam karta hai aur kya waqai aisa soft wear hai bhi ya nahi',\n",
              "  'aap ahbaab mein se ki ghalti durust horahee hai ke ki kaisay kaam karta hai aur kya waqai aisa soft wear hai bhi ya nahi hai ya'),\n",
              " ('Fahim ka Amaar ka mug der se ban kar aay y dono ke taswerain baqyon se kuch choti reh gayeen theen',\n",
              "  'qibla ki list ka visit karne se band kar aeye dono ke rabt tauseef se kuch choti reh gayi .'),\n",
              " ('sabiq wazeer mazhabi umoor Punjab Saeed Al Hasan gilani ne kaha ke quran parhnay ka zouq zindah karne ki zaroorat hai',\n",
              "  'july ke mojooda haqooq punjab islamabad peshawar amman ne kaha ke quran parhnay ka muntazim zindah karne ki zaroorat hai'),\n",
              " ('aap ki pehli ke liye to mujhe Shamshad bhai ki mad leni parre gi uf kitney khtr naak batain kar te hain aap',\n",
              "  'aap ki pehli ke liye to mujhe shamshad bhai ki safaid pari parre gi halaat kitney card ghalti batain kar te hain bakol bakol'),\n",
              " ('America ko zaroorat nahi Pakistan ko pathar ke saal mein le jaane ki hum khud se poanch gaye hain',\n",
              "  'mein ki zaroorat nahi pakistan ko pathar ke saal mein le jaane ki hum khud se poanch gaye hain'),\n",
              " ('takneeki hisaab se to kala baagh dem theek ho sakta hai magar siyasi ya awaami raye kya hai is baray mein',\n",
              "  'jisay hisaab se to garam saal mulazim theek ho sakta hai magar siyasi jung aqwami raye kya hai is baray mein'),\n",
              " ('? me ray aa sipahi / pehla ray aa ko / seen noon kal te / ja te hay',\n",
              "  'tig air , heroen israel ko ajar ke liye ajar gaye ga'),\n",
              " ('milton mesachosist ka Raqba murabba kilomitr hai aur is ki majmoi abadi afraad par mushtamil hai aur meter satah darya se bulandi par waqay hai',\n",
              "  'lankan maghribi lehaaz waqea knsas'),\n",
              " ('Farhat Kiani walidain wala sawal pehlay ho chuka hai . jab taq aap woh talaash karen mein baqi jawab deta hon',\n",
              "  'professor gohar jaisay wala kabhi pehlay ho chuka hai . jab taq aap woh talaash karen mein madad jawab deta hon'),\n",
              " ('aap ne ki bhi dekha hoga ke mein ne khud bohat baar copy paste kya hai lekin mandarja baala sharait par poora utartay hue',\n",
              "  'aap ne ki bhi dekha hoga ke mein ne khud bohat baar copy paste kya hai lekin mandarja baala sharait par poora technology hue'),\n",
              " ('mein bhi Karachi se hon ( kher ki maamla to ke malik ka hai ) aam logon ki terhan',\n",
              "  'mein woh karachi se hon ( kher ki maamla to ke malik ka hai ) taq logon ki'),\n",
              " ('bilkul janab woh se bhi kaafi ouncha guzarta tha agar sirf woh se guzray to shayar pakar bhi laita hai',\n",
              "  'bilkul janab woh se bhi kaafi sa sa tha agar sirf woh se guzray to shayar rahen bhi laita hai'),\n",
              " ('neechay jadole mein hum is masawaat ke takraar ke do silsila likhte hain jo mamooli doori se shuru hotay hain aik jadole mein aur dosray mein hai',\n",
              "  'afghanistan nigaron mein hum is masawaat ke maqsad ke do silsila likhte hain jo jadeed behas se shuru hotay hain aik anjanay mein aur dosray mein hai dosray mein hai'),\n",
              " ('Bharat ka Dehli nah chalay kal klan ko Mohammad Ali jinah ko bhi kisi nah kisi dehsatgardi mein mulawis kardey',\n",
              "  'ke ka dehli nah chalay albata saahil ko mohammad ali jinah ko bhi kisi nah kisi suneney mein peda kardey'),\n",
              " ('pehla rakha acha hai umeed hai rasta bhi jagah ge aur chalne ka is se bhi behtar saleeqa aa jaye ga',\n",
              "  'pehla zamana chalta hai umeed hai talaash bhi aashna ge aur mafhuum ka is se bhi behtar mutawaqqa aa jaye ga'),\n",
              " ('ab chunkay aaj ki jagah ko bohat ziyada adaad ki durusti se pemaiesh karna mumkin nahi is liye hazaar saal baad ka jawab maloom nahi ho sakta',\n",
              "  'ab chunkay aaj ki jagah ko bohat ziyada malik ki nigah se rasai karna mumkin nahi is liye hazaar saal baad ka jawab maloom nahi ho sakta ho'),\n",
              " ('acha ki aap ne tanz kya hai kyunkay aap se tanz ke ilawa koy umeed ki bhi nahi ja sakti',\n",
              "  'kya is ke ne arz kya hai kyunkay aap se surkhab ke ilawa ko umeed ki bhi nahi ja sakti hon'),\n",
              " ('Akhtar shirani ki ghazlon mein riwayati mazameen ja baja dikhayi dete hain aur beshtar naazon mein bandhay gay mazameen bhi ghazlon mein Dur atay hain',\n",
              "  'ya tahir ki ghazlon mein mustaqil mazameen nah dikhayi dete hain aur quresh aknaaf mein mein gay november bhi hamza mein se sahara likhte'),\n",
              " ('aur un ke ( apne gharrey hue ) shareek ( un se ) kahin ge ke tum hamari ibadat to nahi baghaawat they',\n",
              "  'aur ke ke ( apne antrah hue ) laip ( un se ) kahin ge ke tum hamari karkardagi to nahi baghaawat they'),\n",
              " ('aisay harf jin mein hamza alehda harf ke tor par istemaal hota hai un ki kuch misalein darj zail hain',\n",
              "  'aisay haftay jin mein mahana feesad harf ke tor par istemaal hota hai un ki kuch nuqsaan darj zail hain'),\n",
              " ('agarchay is nazam ki ashaat se shayad ki mutadid bah se society ko nahi pouncha magar chay baras mein jis',\n",
              "  'tom is pedawar ki fehrist se shayad koi alfaaz taq science brian ko nahi kya magar chay baras mein new so paashi'),\n",
              " ('qabar aur mamu ke darmain wali deewar ke darwazay mein kharray ho kar mamu ki aur sehan ki tasaveer banayen',\n",
              "  'jinhein aur mamu ke darmain aman college ke darwazay mein kharray ho kar mamu ki aur mafadaat ki tasaveer banaye'),\n",
              " ('mein ik baar sahih ki bemaar hui bhi hostel mein tab lag pata gaya tha ke mamu se saal bemaar hona kaisa hota hai',\n",
              "  'mein ik baar sahih ki bemaar hui bhi baabo mein tab lag pata gaya tha ke mamu se saal hue hona aasaan hota hai'),\n",
              " ('aur jee zia ki goad mein pal kar jawan honay walay kar ko is ke dast baazu honay ka tana dete hain . kya kehnay .',\n",
              "  'aur jee mouse ki aarr mein peda kar atay honay walay kar ko is ke pakkay pari honay ka ishtihar dete hain . kya na .'),\n",
              " ('federal rizro ne saloon ke baad aakhir - kaar decemeber ko sharah sood mein izafah kya magar sirf feesad',\n",
              "  'lankan maghribi hissay baad baad iraq takhat khan ne mutafiqa afraad mein izafah kya thi jabkay meter tha'),\n",
              " ('afsanwi majmuon aur navlon ke ilawa tanqeed tahaqeeq aur deegar kayi mozuaat par dr Saleem Akhtar ki mutadid kitaaben shaya ho chuki hain',\n",
              "  'malik se aur mualim ke ilawa tanqeed tahaqeeq aur deegar kayi garohoon par dr abwalkhir hussain ki mutadid kitaaben shaya ho chuki hain'),\n",
              " ('is ko ginti mein paintalis bola jata hai is se pehlay chawalees aur is ke baad chhyalis bola jata hai',\n",
              "  'is ko ginti mein shumali bola jata hai is se pehlay syedna aur is ke baad terah bola jata hai'),\n",
              " ('taham ab taq yeh taayun nahi kya jaska hai yeh dhamaka janoobi Mumbai mein jari karwayyon ka tasalsul hai ya koi allag noiyat ka waqea',\n",
              "  'taham ab taq yeh par nahi kya apni hai ki mareez ke dubai mein jari karwai ka tasalsul hai ya yeh allag zail ka raqba murabba raqba raqba murabba'),\n",
              " ('Germany ke aik aisay shehar mein jis ki aqwami aik zamane mein barri mashoor rahi he waisay pata chal bhi jaye to naam sukh lijiye ga',\n",
              "  'america ke aik aisay shehar mein jis ki aqwami aik zamane mein barri jhalak rahi he waisay pata chal bhi jaye to naam sukh aeye ga'),\n",
              " ('quied Hussain ke mamu Haq bhai Saeed varsi ke mamu Haq bhai Saeed varsi ka soyam aaj hoga',\n",
              "  'kawish : ke mamu , bhai ( november ke mamu , bhai 5 bank ka pehla mein aaj hoga'),\n",
              " ('ki saari is baat ki nishanain theen ke ab is ne mein baatil ki farmanrawai aur mukammal tehreer parheen',\n",
              "  'ke bawajood sab thi ki likhi theen ke ab is ne mein insidaad ki awaami team aur mukammal tehreer pa pa'),\n",
              " ('Mosoof bta rahay hain ke woh ghalti se sarhad paar kar ke aa gaye they aur be gunah hain',\n",
              "  'ke socha rahay hain ke woh ghalti se fi amal kar ke liye gaye they aur be gunah hain'),\n",
              " ('mein ne kabhi ki socha hi nahi Dehli jab shukriya aa jaye to sochna band ho jata hai',\n",
              "  'pakistan ne kabhi yeh socha hi nahi dehli jab chhota aa jaye to hukum band ho jata hai'),\n",
              " ('unhon ne kaha ke aloodgi globl varmng aur pani ki aloodgi ki islam ne so saal qabal baat ki thi',\n",
              "  'unhon ne kaha ke goreela nigari baqiyat aur pani ki jasoosi ki islam ne badshah saal qabal thi ki thi'),\n",
              " ('lehaza is tanzeem ne Pakistan ke liye pi ke ke alfaaz mutayyan kiye hain jo Pakistan ko zahir karta hai',\n",
              "  'hai is terhan do parast aur aik raye ki ghair janbdaranh bani raha hoti hai'),\n",
              " ('sita haal Pakistani nah sahi magar uska ki claim to durust hai ke uski bachi ka baap Pakistani hai',\n",
              "  'kawish ki pakistani nah hon magar aik ki behnain to durust hai ke aik pegham ka kitney pakistani hai'),\n",
              " ('pahari ilaqa honay ki wajah se zaraat mehdood hai ziyada tar zarayi pedawar mein Zaitoon phal aur phool shaamil hain',\n",
              "  'mshi gun honay ki wajah se bulandi par hai ziyada tar zarayi darya mein zam phal aur nateeja mein hain'),\n",
              " ('mein government high school shikho poora se metric kya aur Sadiq ayjrtn college bahawalpoor mein daakhil hogaye jahan se mein bi',\n",
              "  'mein sabiq director justice 51 taq se metric kya aur shikast aknaaf college bahawalpoor mein daakhil hogaye jahan se mein jaan diye'),\n",
              " ('kher asal masla ki hai ke chunkay ab multi national companian Balochistan mein apna asro rasookh barha rahi hain churraya aisay chtkle chodthee rehti hain !',\n",
              "  'kher aik masla ki hai ke chunkay ab hoor national 40 market mein apna hassas temen mutfiq rahi hain churraya kuch doghlay ikhatay rehti hain !'),\n",
              " ('jab ki user anjanay mein ki bhi aisa lafz likhta he to wahan woh lafz ki jagah aisi ban jaye ga',\n",
              "  'jab ki daku pehlay mein ki bhi aisa lafz rehta he to wahan woh lafz ki jagah wali ban jaye ga'),\n",
              " ('chay nakaat par par baghaawat hue yahan mahol ko jayen banaya jaye to balouch qoum hukmraanon ke sath mazakraat ko liye hai',\n",
              "  'mohammad awwal 1940 par qabza karte hue yahan november ko golah banaya jaye to balouch qoum mangnay ke sath arkaan ko liye'),\n",
              " ('zabardast photography aur manazair ke umdah akkaasi - mehfhil ke tasaveer ke zamray mein to aap aur askari chaaye hue hain',\n",
              "  'se hansa aur akhbarat ke baais - - sahib ke bande ke zamray mein to aap aur mafadaat gaye hue hain'),\n",
              " ('aur ki ke msjdin ( khaas ) kkhuda ki hain to kkhuda ke sath kisi aur ki ibadat nah karo',\n",
              "  'aur yeh ke baani ( khaas ) kkhuda ki hain to kkhuda ke sath kisi aur ki ibadat nah nah'),\n",
              " ('acha to is ka matlab hai ke woh das tasweer ki dastanoon mein kuch nah kuch haqeeqat hai !',\n",
              "  'acha to is ka matlab hai ke woh aqwam goi ki tahreeron mein kuch nah kuch haqeeqat hai !'),\n",
              " ('is ke zaiqay mein aaj taq kisi qisam ki tabdeeli waqay nahi hui balkay roz awwal se aaj taq is ka wohi zayeqa hai',\n",
              "  'is ke daira mein aaj taq kisi qisam ki himayat rakhta nahi hui balkay aath awwal se aaj taq is ka laazmi library hai'),\n",
              " ('aasaan alfaaz mein usay yun bayan kya ja sakta hai ke aik dawat mein so maheman hain aur so rotian hain',\n",
              "  'fehrist ki mein usay yun bayan kya ja sakta hai ke aik pal mein sitara aajate hain aur dar -'),\n",
              " ('is waqt se kayi buland amarat ke mansoobay manzar aam par aachukay hain jin mein se chand zair taamer bhi hain',\n",
              "  'is khittay se kayi neher tarameem ke douran manzar aam par zair hain jin mein se chand zair taamer thi'),\n",
              " ('abhi woh kaam shuru hi kar tha ke achanak computer par aik e mil khil gayi jis mein ki hadees likhi bhi',\n",
              "  'abhi woh kaam shuru hi kar tha ke achanak computer par aik e mil poanch gayi jis mein ki hifazat likhi thi'),\n",
              " ('is ke sath sath hum un se poucheen ge un ki pasand / napasand un ki ko shakhsiyat ke baray mein sawalaat aur dhair saari batain',\n",
              "  'is ke sath sath hum un se poucheen ge un ki pasand pari hoon un ki ko shakhsiyat ke baray mein sawalaat aur dhair saari batain'),\n",
              " ('aur is se pehlay do hazaar aik so athanway ya ikees so athanway aur is ke baad do hazaar do so ya baaes so bola jata hai',\n",
              "  'aur is se pehlay do hazaar aik so unchaas ya unatees so artalees aur is ke baad do hazaar do so ya chabis so bola jata hai'),\n",
              " ('phir se bore ab urdu mehfhil mein pehlay sa josh nahi raha aur nah hi uskay josh lotney ki umeed hai ab',\n",
              "  'phir se jaahil ab urdu mehfhil mein pehlay sa reham nahi raha aur nah hi uskay tamam taswerain ki umeed hai ab'),\n",
              " ('kyunkay mein be penday ka lota hon har taraf ghoom jaun ga aur baala aakhir - kaar bachay ga kon be penday ka lota',\n",
              "  'kyunkay mein be aankhon ka purana hon har taraf bol jaoon ga aur 15 aakhir - kaar uncle ga kkhuda mera rawaangi ka'),\n",
              " ('is nizaam mein har shakhs ki aamdani se mehsool wusool kya jata hai chahay woh ghareeb ho ya maldaar',\n",
              "  'is ka shumaar har shakhs ki aamdani se technology wusool kya jata hai chahay woh dobarah ho ya'),\n",
              " ('marshal New York ka Raqba murabba kilomitr hai aur is ki majmoi abadi afraad par mushtamil hai aur meter satah samandar se bulandi par waqay hai',\n",
              "  'marshal new york ka raqba murabba kilomitr hai aur is ki majmoi abadi afraad par mushtamil hai aur meter satah samandar se bulandi par waqay hai'),\n",
              " ('aur aakhir - kaar mein aanar ke danay daal kar jaldi se kha len aisa nah ho ke ki aajay',\n",
              "  'aur aakhir - kaar mein amitabh ke awleen injaam kar jaldi se kha len aisa nah ho ke ki aayi'),\n",
              " ('agar hum is doosri wahi ke qaail nahi hain to phir mujhe kehnay dijiye ke bakol quran',\n",
              "  'agar un is doosri mutalbe ke qaail nahi hain to phir mujhe chhota dijiye ke bakol allah'),\n",
              " ('bees hazaar maraslay to waqai aisa sang mil jis par mubarakbaad wajib hojati hai hamari mubarakbaad ke baghair ki dhaga gayarah safhaat taq poanch gaya !',\n",
              "  'bees hazaar mein to sahih aisa sang mil jis par mubarakbaad lana hojati hai hamari raat ke baghair ki dhaga ya saal taq poanch gaya !'),\n",
              " ('America ke baaz mahireen ke mutabiq chain ne laser shua ki madad se Amrici jasoosi sayyaron ko andha karne ki koshish bhi ki hai',\n",
              "  'america ke zariye shehri ke mutabiq chain ne australia aliktranks ki madad se amrici shart company ko manzoor karne ki koshish thi ki hai thi ki hai'),\n",
              " ('akhbar injaam ko shikast dainay ke liye sansani khaiz mawaad jinsi aur juraim par mabni kahaniyan aur aurat ka jism akhbar ki mustaqil khasusiyat bana raha',\n",
              "  'college coat ko shikast dainay ke qaza khaiz mawaad ki aur qabail car par market mein aur aik ka raqba alkht ki intahaa mumkin bana raha'),\n",
              " ('aur Pakistanio ke liye napasandeedah tareen America hai . agar pehlay number se pehlay bhi ki number hota to mein un ko hi deta .',\n",
              "  'aur jab ke liye majboron tareen rupay hai . agar pehlay number se pehlay bhi ki number hota to mein un ko hi deta .'),\n",
              " ('bhai par ka link lagta hai kaam nahi kar raha agar theek kar den to mein bhi nazar ada kar dun',\n",
              "  'mohammad may se mein shaya hai kaam nahi kar raha agar theek kar den to mein bhi nazar ada kar dun gi'),\n",
              " ('ho nahi sakta balkay usay kal taq ho jana chahiye ! Imran ne aik aik lafz par zor day kar kaha !',\n",
              "  'ho raha hon ke kuch kal taq ho jana chahiye ! imran ne aik aik lafz par zor day kar kaha !'),\n",
              " ('Shamshad bhai aa chuke hain aap ka salam mil gaya ke salam bohat nazar bhai alhamdulillah sab acha hai aap sunaein kaisay hain',\n",
              "  'shamshad bhai aa chuke hain aap ka saathi mil gaya ke salam bohat nazar bhai kisay sab acha hai aap sunaein kaisay'),\n",
              " ('port gbsn msispi ka Raqba murabba kilomitr hai aur is ki majmoi abadi afraad par mushtamil hai aur meter satah samandar se bulandi par waqay hai',\n",
              "  'lake farst vashngtn ka raqba murabba kilomitr hai aur is ki majmoi abadi afraad par mushtamil hai aur meter satah samandar se bulandi par waqay hai'),\n",
              " ('college jung aur jung se mutaliqa aloom ka aalmi shohrat rakhnay wala idaara hai jis mein back waqt afsaraan taleem haasil karte hain',\n",
              "  'college ya aur jung se mutaliqa muslim ka aalmi shohrat rakhnay wala azad hai jis mein back waqt se taleem haasil karte hain'),\n",
              " ('un tamam filmon mein Surriya ko hi ahem kirdaar ada karne ka mauqa mila jis se yeh alamat zahir honay lagi ke woh dio se barri adakara hain',\n",
              "  'un tamam filmon mein mehsoolat ko hi ahem kirdaar ada karne ka mauqa mila jis se yeh barhti zahir honay lagi ke woh afaq se barri maqbool hain'),\n",
              " ('sdraldin ko to bohat ki aayi jab hum ne inhen bataya kahoon hain uncle ko kis ne kaha tha jald baazi karne ko acha sun - hwa ab',\n",
              "  'humsaiyon ko to bohat ki aayi jab hum ne inhen bataya kahoon hain ge ko kis ne kaha jahan jald baazi karne ko acha sun - hwa hai'),\n",
              " ('Yemen teesri ne ka aik intahi pasmandah malik hai jo Ameer khliji mumalik ka Afghanistan kaha ja sakta hai',\n",
              "  'middle is duniya ka aik kayi companian malik hai jo general kasmiri mumalik ka aaghaz kaha ja sakta hai'),\n",
              " ('is baichari boorhi ne pata nahi kya kya dekha ho ga zamane mein jo ki itni barri baat nahi bhi',\n",
              "  'is churail kiijiye ne pata nahi kya kya jawab ho ga mein mein jo ki itni barri baat nahi bhi'),\n",
              " ('plat aur mera Allah aap ki zabaan mubarak kere ki do inch zamee Pakistan mein hoti to barson idhar nah rehta',\n",
              "  'kawish aur mera allah aap ki zabaan mubarak kere koi aur inqilab asif pakistan mein hoti to idhar idhar nahi rehta'),\n",
              " ('is kitaab haqeeqat toheed o tamam ko aan line parhnay ya down load karne ke liye yahan click karen',\n",
              "  'is kitaab tareekh insaaf o tamam ko aan line parhnay ya down load karne ke liye yahan click karen gi'),\n",
              " ('asbaq tareekh Nami ki kitaab aik aik safhay ke mukhtasir mazameen ka majmoa hai jis mein sahaba ke saal se le kar agay ke adwaar par',\n",
              "  'sahaafat tareekh bnars ki kitaab aik aik ilaqay ke makhsoos mazameen ka majmoa hai jis mein sultan ke daur se le kar khud ke adwaar par di karen'),\n",
              " ('is hamlay ke liye hajhoom ko uksanay ka kaam muqami bhartia jnta party ke rukan ke betay ne kya tha',\n",
              "  'is hamlay ke liye hajhoom ko hig ka kaam aur urinium aqwam hazaar ke rukan ke logon ne kya tha'),\n",
              " ('aik klov meter ya is se ziyada to phir ne liye behtar rahay ga is baray mein kuch yeh kitna kharch aeye ga',\n",
              "  'aik nazam meter ya is se ziyada to phir ne liye behtar rahay ga is baray mein kuch yeh yahin daur laga ga'),\n",
              " ('ke jab quran ke naam par nojawan angrezi taleem Yafta aur islam al tabqa ko ki bawar karaya jata hai',\n",
              "  'ke jab sultan ke naam par awam laakh taleem yafta aur islam baagh tabqa ko yeh shafaat samgha jata hai'),\n",
              " ('dr mubarak mubarak mand ne kaha ke thar ke koylay ke zakhair tawanai ke husool ke liye 800 saal taq ke liye kaafi hain -',\n",
              "  'dr 7 mubarak qawaneen ne kaha ke denmark ke ko le ke nai karwai ke husool ke liye 40 saal taq ke liye kaafi hain'),\n",
              " ('mein ne likha tha ke unwan mein hi jawab hai phir tadween kar di ke kahin ghalib shamat na le ayen meri',\n",
              "  'mein ne likha tha ke unwan mein hi jawab hai phir tadween kar di ke kahin daagh na na le mujh'),\n",
              " ('ne mamu mein hai to mujh se chhupa ke rakhtay hain ya ziyada le ke se hain ke ke to yahi kha jaye ga',\n",
              "  'ne mamu mein hai to mujh se doodh ke rakhtay hain ya ziyada le ke se hain ke ke to yahi kahe jaye ga'),\n",
              " ('is ke ilawa inhen sahtih academy award aur deegar ezazat bhi haasil ho ye jin mein punjabi adab ke liye geyan pith award bhi shaamil hai',\n",
              "  'is ke ilawa inhen e ae shaheed aur deegar ezazat bhi haasil hue jin mein german adab ke liye matawazi takmeeli award bhi shaamil hai'),\n",
              " ('ya is terhan ki mein millti maloomat jo ke aap ke khayaal mein vayrlis net work ke liye itni hon zaroor yeh',\n",
              "  'ya is terhan ki mein millti hain jo ke aap ke khayaal mein faiz awaien net bharnay ke liye thori nazar zaroor yeh'),\n",
              " ('chalein yahan bhi day deti hon bohat bohat mubarak ho aur Allah kere ke un saloon ki tadaad yuhin barhti rahay',\n",
              "  'haan yahan bhi day deti hon bohat bohat mubarak ho aur allah kere ke un saloon ki tadaad awam karli parre')]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# calculate the prediction\n",
        "preds_bucket_greedy = predict_greedy(test_buckets[current_index][0:100])\n",
        "preds_bucket_greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwG2qsPJdBmL",
        "outputId": "6c1428e9-6c37-4e79-f196-170dad668ca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "greedy search\n",
            "BLEU SCORE OF BUCKET SIZE 30: 49.704764298910376\n",
            "ROUGE SCORE OF BUCKET SIZE 30: {'rouge1': 0.7172959329019077, 'rouge2': 0.5625307790125281, 'rougeL': 0.7164561638796303, 'rougeLsum': 0.7160805815953828}\n"
          ]
        }
      ],
      "source": [
        "# quantitaive analysis - bleu score of model, rouge score\n",
        "print('greedy search')\n",
        "bleu_score = calculate_bleu_score(preds_bucket_greedy)\n",
        "rouge = evaluate.load('rouge')\n",
        "expected = [pair[0] for pair in preds_bucket_greedy]\n",
        "predicted = [pair[1] for pair in preds_bucket_greedy]\n",
        "rouge_score = rouge.compute(predictions=predicted, references=expected)\n",
        "print(\"BLEU SCORE OF BUCKET SIZE 30:\", bleu_score)\n",
        "print(\"ROUGE SCORE OF BUCKET SIZE 30:\", rouge_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WbjheUZ2lCF",
        "outputId": "b08338c2-84a4-480c-b297-f88cfa4a1435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Urdu Sentence: لیکن اس نظم کو پڑھ کر یہ نتیجہ نکالا ہے ہم نے کہ چار کتابیں پڑھنا تو بے حد اسان ہے\n",
            "Actual Roman Urdu Sentence: lekin is nazam ko parh kar ki nateeja nikala hai hum ne ke chaar kitaaben parhna to be koi aasaan hai\n",
            "Predicted Roman Urdu Sentence: lekin is nazam ko parh kar ki koshish mila hai hum ne ke chaar kitaaben pasand to be koi aasaan hai\n",
            "\n",
            "Urdu Sentence: چلیں یہاں بھی دے دیتی ہوں بہت بہت مبارک ہو اور اللہ کرے کہ ان سالوں کی تعداد یونہی بڑھتی رہے\n",
            "Actual Roman Urdu Sentence: chalein yahan bhi day deti hon bohat bohat mubarak ho aur Allah kere ke un saloon ki tadaad yuhin barhti rahay\n",
            "Predicted Roman Urdu Sentence: haan yahan bhi day deti hon bohat bohat mubarak ho aur allah kere ke un saloon ki tadaad awam karli parre\n",
            "\n",
            "Urdu Sentence: چلیں یہاں بھی دے دیتی ہوں بہت بہت مبارک ہو اور اللہ کرے کہ ان سالوں کی تعداد یونہی بڑھتی رہے\n",
            "Actual Roman Urdu Sentence: chalein yahan bhi day deti hon bohat bohat mubarak ho aur Allah kere ke un saloon ki tadaad yuhin barhti rahay\n",
            "Predicted Roman Urdu Sentence: haan yahan bhi day deti hon bohat bohat mubarak ho aur allah kere ke un saloon ki tadaad awam karli parre\n",
            "\n",
            "Urdu Sentence: ڈاکٹر ثمر مبارک مند نے کہا کہ تھر کے کو لے کے ذخا ر توانا ی کے حصول کے لیے 800 سال تک کے لیے کافی ہیں -\n",
            "Actual Roman Urdu Sentence: dr mubarak mubarak mand ne kaha ke thar ke koylay ke zakhair tawanai ke husool ke liye 800 saal taq ke liye kaafi hain -\n",
            "Predicted Roman Urdu Sentence: dr 7 mubarak qawaneen ne kaha ke denmark ke ko le ke nai karwai ke husool ke liye 40 saal taq ke liye kaafi hain\n",
            "\n",
            "Urdu Sentence: 3 . مگر ( اسے ) اس شخص کے ل ے نصیحت ( بنا کر اتارا ) ہے جو ( اپنے رب سے ) ڈرتا ہے\n",
            "Actual Roman Urdu Sentence: 3 . magar ( usay ) is shakhs ke liye nasiyaat ( bana kar utaara ) hai jo ( apne rab se ) darta hai\n",
            "Predicted Roman Urdu Sentence: 14 . magar ( usay ) is shakhs ke liye nasiyaat ( bana kar ahthyat ) hai jo ( apne rab se ) dekhatii hai\n",
            "\n",
            "Urdu Sentence: اس کے ذا قے میں اج تک کسی قسم کی تبدیلی واقع نہیں ہو ی بلکہ روز اول سے اج تک اس کا وہی ذا قہ ہے\n",
            "Actual Roman Urdu Sentence: is ke zaiqay mein aaj taq kisi qisam ki tabdeeli waqay nahi hui balkay roz awwal se aaj taq is ka wohi zayeqa hai\n",
            "Predicted Roman Urdu Sentence: is ke daira mein aaj taq kisi qisam ki himayat rakhta nahi hui balkay aath awwal se aaj taq is ka laazmi library hai\n",
            "\n",
            "Urdu Sentence: یہ ایک مکمل قران کا لنک ہے اسے خود بھی تلاوت کریں اور اپنے دوسرے اسلامی بھا یوں کو بھی اسے تلاوت کرنے کی ترغیب دلا یں\n",
            "Actual Roman Urdu Sentence: ki aik mukammal quran ka link hai usay khud bhi talawat karen aur apne dosray islami bhaieyon ko bhi usay talawat karne ki targheeb delaina\n",
            "Predicted Roman Urdu Sentence: ki aik qanoon quran ka elaan hai usay khud bhi jali karen aur apne dosray ke haqayiq ko bhi usay e koshish ki targheeb\n",
            "\n",
            "Urdu Sentence: اگر ہم اس دوسری وحی کے قا ل نہیں ہیں تو پھر مجھے کہنے دیج ے کہ بقول قران\n",
            "Actual Roman Urdu Sentence: agar hum is doosri wahi ke qaail nahi hain to phir mujhe kehnay dijiye ke bakol quran\n",
            "Predicted Roman Urdu Sentence: agar un is doosri mutalbe ke qaail nahi hain to phir mujhe chhota dijiye ke bakol allah\n",
            "\n",
            "Urdu Sentence: میں اک بار صحیح کی بیمار ہو ی تھی ہاسٹل میں تب لگ پتہ گیا تھا کہ گھر سے دور بیمار ہونا کیسا ہوتا ہے\n",
            "Actual Roman Urdu Sentence: mein ik baar sahih ki bemaar hui bhi hostel mein tab lag pata gaya tha ke mamu se saal bemaar hona kaisa hota hai\n",
            "Predicted Roman Urdu Sentence: mein ik baar sahih ki bemaar hui bhi baabo mein tab lag pata gaya tha ke mamu se saal hue hona aasaan hota hai\n",
            "\n",
            "Urdu Sentence: جو شعر میں نے لکھا ہے وہ استاد مومن خان مومن کا تمہارے پچھواڑے میں جو مومن گویا ہے اس کا نہیں\n",
            "Actual Roman Urdu Sentence: jo shair mein ne likha hai woh ustaad momin Khan momin ka tumahray barah mein jo momin goya hai is ka nahi\n",
            "Predicted Roman Urdu Sentence: jo shair mein ne likha hai woh ustaad momin khan kazmi ka apne mukalama mein jo hindi boltaa hai is ka\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# qualitative analysis - lets see a few example sentences and their transliterations\n",
        "# since greedy gave better performance on quant. metrics, we are just showing the sentences using greedy approach.\n",
        "for i in range(10):\n",
        "  inx = random.randint(0, len(preds_bucket_greedy))\n",
        "  r, u = test_buckets[current_index][inx]\n",
        "  print('Urdu Sentence:', u.replace('<start>', '').replace('<end>', '').strip())\n",
        "  print('Actual Roman Urdu Sentence:', r.replace('<start>', '').replace('<end>', '').strip())\n",
        "  orig, pred = preds_bucket_greedy[inx]\n",
        "  print('Predicted Roman Urdu Sentence:', pred.replace('<start>', '').replace('<end>', '').strip())\n",
        "  print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0dc8f6900fc04ecea590ef06686f7848": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a00c7cb101cd4ccd9909e2ef7204f581",
            "placeholder": "​",
            "style": "IPY_MODEL_9c4a1fc1291b4d47a69915ca40efb9e3",
            "value": "Downloading builder script: 100%"
          }
        },
        "356278234f6d4b8ebca3302569f11c79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3af102dbd85b4b4dbde66c7a1c0ffa31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0dc8f6900fc04ecea590ef06686f7848",
              "IPY_MODEL_aa031f1647be40b5aa27ca007cc468dc",
              "IPY_MODEL_7fc837eae5654b88997eb225d37a9ebf"
            ],
            "layout": "IPY_MODEL_cd3acb96a2dd4246a478c1ae1409f4d0"
          }
        },
        "6a3052ed7f3c43afb9ac7d38afbb18d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fc837eae5654b88997eb225d37a9ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa236bb411de48beaf38ceef03f62999",
            "placeholder": "​",
            "style": "IPY_MODEL_6a3052ed7f3c43afb9ac7d38afbb18d7",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 406kB/s]"
          }
        },
        "81a89f4406d14f0ea87703355e5da616": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c4a1fc1291b4d47a69915ca40efb9e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a00c7cb101cd4ccd9909e2ef7204f581": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa031f1647be40b5aa27ca007cc468dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81a89f4406d14f0ea87703355e5da616",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_356278234f6d4b8ebca3302569f11c79",
            "value": 6270
          }
        },
        "aa236bb411de48beaf38ceef03f62999": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd3acb96a2dd4246a478c1ae1409f4d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
